
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Capstone Project Details and Deliverables &#8212; CSE 4/573 - Computer Vision and Image Processing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/thebe.css?v=9bca0c2f" />
    <link rel="stylesheet" type="text/css" href="_static/code.css?v=1719a5f1" />
    <link rel="stylesheet" type="text/css" href="_static/image_dark_mode.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx_iframe.css" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="_static/tippy.css" />
    <link rel="stylesheet" type="text/css" href="_static/tb_cc.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/fix_admonition_style.css?v=8627ce31" />
    <link rel="stylesheet" type="text/css" href="_static/fix_dropdown_style.css?v=f3461767" />
    <link rel="stylesheet" type="text/css" href="_static/fix_code_header_style.css?v=7d542921" />
    <link rel="stylesheet" type="text/css" href="_static/color.css?v=fb14535a" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script src="_static/h5p-resizer.js"></script>
    <script defer="defer" src="_static/refresh.js?v=9bea9b76"></script>
    <script>const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script defer="defer" src="_static/sphinx-thebe-lite.js?v=94e8009d"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script defer="defer" src="_static/tippy/capstone-project.38f5f07c-66c9-4d18-af8b-07f9bb1b7844.js"></script>
    <script>window.MathJax = {"chtml": {"mtextInheritFont": true}, "tex": {"macros": {"black": ["\\class{black}{#1}", 1], "silver": ["\\class{silver}{#1}", 1], "gray": ["\\class{gray}{#1}", 1], "white": ["\\class{white}{#1}", 1], "maroon": ["\\class{maroon}{#1}", 1], "red": ["\\class{red}{#1}", 1], "purple": ["\\class{purple}{#1}", 1], "fuchsia": ["\\class{fuchsia}{#1}", 1], "green": ["\\class{green}{#1}", 1], "lime": ["\\class{lime}{#1}", 1], "olive": ["\\class{olive}{#1}", 1], "yellow": ["\\class{yellow}{#1}", 1], "navy": ["\\class{navy}{#1}", 1], "blue": ["\\class{blue}{#1}", 1], "teal": ["\\class{teal}{#1}", 1], "aqua": ["\\class{aqua}{#1}", 1], "aliceblue": ["\\class{aliceblue}{#1}", 1], "antiquewhite": ["\\class{antiquewhite}{#1}", 1], "aquamarine": ["\\class{aquamarine}{#1}", 1], "azure": ["\\class{azure}{#1}", 1], "beige": ["\\class{beige}{#1}", 1], "bisque": ["\\class{bisque}{#1}", 1], "blanchedalmond": ["\\class{blanchedalmond}{#1}", 1], "blueviolet": ["\\class{blueviolet}{#1}", 1], "brown": ["\\class{brown}{#1}", 1], "burlywood": ["\\class{burlywood}{#1}", 1], "cadetblue": ["\\class{cadetblue}{#1}", 1], "chartreuse": ["\\class{chartreuse}{#1}", 1], "chocolate": ["\\class{chocolate}{#1}", 1], "coral": ["\\class{coral}{#1}", 1], "cornflowerblue": ["\\class{cornflowerblue}{#1}", 1], "cornsilk": ["\\class{cornsilk}{#1}", 1], "crimson": ["\\class{crimson}{#1}", 1], "cyan": ["\\class{cyan}{#1}", 1], "darkblue": ["\\class{darkblue}{#1}", 1], "darkcyan": ["\\class{darkcyan}{#1}", 1], "darkgoldenrod": ["\\class{darkgoldenrod}{#1}", 1], "darkgray": ["\\class{darkgray}{#1}", 1], "darkgreen": ["\\class{darkgreen}{#1}", 1], "darkgrey": ["\\class{darkgrey}{#1}", 1], "darkkhaki": ["\\class{darkkhaki}{#1}", 1], "darkmagenta": ["\\class{darkmagenta}{#1}", 1], "darkolivegreen": ["\\class{darkolivegreen}{#1}", 1], "darkorange": ["\\class{darkorange}{#1}", 1], "darkorchid": ["\\class{darkorchid}{#1}", 1], "darkred": ["\\class{darkred}{#1}", 1], "darksalmon": ["\\class{darksalmon}{#1}", 1], "darkseagreen": ["\\class{darkseagreen}{#1}", 1], "darkslateblue": ["\\class{darkslateblue}{#1}", 1], "darkslategray": ["\\class{darkslategray}{#1}", 1], "darkslategrey": ["\\class{darkslategrey}{#1}", 1], "darkturquoise": ["\\class{darkturquoise}{#1}", 1], "darkviolet": ["\\class{darkviolet}{#1}", 1], "deeppink": ["\\class{deeppink}{#1}", 1], "deepskyblue": ["\\class{deepskyblue}{#1}", 1], "dimgray": ["\\class{dimgray}{#1}", 1], "dimgrey": ["\\class{dimgrey}{#1}", 1], "dodgerblue": ["\\class{dodgerblue}{#1}", 1], "firebrick": ["\\class{firebrick}{#1}", 1], "floralwhite": ["\\class{floralwhite}{#1}", 1], "forestgreen": ["\\class{forestgreen}{#1}", 1], "gainsboro": ["\\class{gainsboro}{#1}", 1], "ghostwhite": ["\\class{ghostwhite}{#1}", 1], "gold": ["\\class{gold}{#1}", 1], "goldenrod": ["\\class{goldenrod}{#1}", 1], "greenyellow": ["\\class{greenyellow}{#1}", 1], "grey": ["\\class{grey}{#1}", 1], "honeydew": ["\\class{honeydew}{#1}", 1], "hotpink": ["\\class{hotpink}{#1}", 1], "indianred": ["\\class{indianred}{#1}", 1], "indigo": ["\\class{indigo}{#1}", 1], "ivory": ["\\class{ivory}{#1}", 1], "khaki": ["\\class{khaki}{#1}", 1], "lavender": ["\\class{lavender}{#1}", 1], "lavenderblush": ["\\class{lavenderblush}{#1}", 1], "lawngreen": ["\\class{lawngreen}{#1}", 1], "lemonchiffon": ["\\class{lemonchiffon}{#1}", 1], "lightblue": ["\\class{lightblue}{#1}", 1], "lightcoral": ["\\class{lightcoral}{#1}", 1], "lightcyan": ["\\class{lightcyan}{#1}", 1], "lightgoldenrodyellow": ["\\class{lightgoldenrodyellow}{#1}", 1], "lightgray": ["\\class{lightgray}{#1}", 1], "lightgreen": ["\\class{lightgreen}{#1}", 1], "lightgrey": ["\\class{lightgrey}{#1}", 1], "lightpink": ["\\class{lightpink}{#1}", 1], "lightsalmon": ["\\class{lightsalmon}{#1}", 1], "lightseagreen": ["\\class{lightseagreen}{#1}", 1], "lightskyblue": ["\\class{lightskyblue}{#1}", 1], "lightslategray": ["\\class{lightslategray}{#1}", 1], "lightslategrey": ["\\class{lightslategrey}{#1}", 1], "lightsteelblue": ["\\class{lightsteelblue}{#1}", 1], "lightyellow": ["\\class{lightyellow}{#1}", 1], "limegreen": ["\\class{limegreen}{#1}", 1], "linen": ["\\class{linen}{#1}", 1], "magenta": ["\\class{magenta}{#1}", 1], "mediumaquamarine": ["\\class{mediumaquamarine}{#1}", 1], "mediumblue": ["\\class{mediumblue}{#1}", 1], "mediumorchid": ["\\class{mediumorchid}{#1}", 1], "mediumpurple": ["\\class{mediumpurple}{#1}", 1], "mediumseagreen": ["\\class{mediumseagreen}{#1}", 1], "mediumslateblue": ["\\class{mediumslateblue}{#1}", 1], "mediumspringgreen": ["\\class{mediumspringgreen}{#1}", 1], "mediumturquoise": ["\\class{mediumturquoise}{#1}", 1], "mediumvioletred": ["\\class{mediumvioletred}{#1}", 1], "midnightblue": ["\\class{midnightblue}{#1}", 1], "mintcream": ["\\class{mintcream}{#1}", 1], "mistyrose": ["\\class{mistyrose}{#1}", 1], "moccasin": ["\\class{moccasin}{#1}", 1], "navajowhite": ["\\class{navajowhite}{#1}", 1], "oldlace": ["\\class{oldlace}{#1}", 1], "olivedrab": ["\\class{olivedrab}{#1}", 1], "orange": ["\\class{orange}{#1}", 1], "orangered": ["\\class{orangered}{#1}", 1], "orchid": ["\\class{orchid}{#1}", 1], "palegoldenrod": ["\\class{palegoldenrod}{#1}", 1], "palegreen": ["\\class{palegreen}{#1}", 1], "paleturquoise": ["\\class{paleturquoise}{#1}", 1], "palevioletred": ["\\class{palevioletred}{#1}", 1], "papayawhip": ["\\class{papayawhip}{#1}", 1], "peachpuff": ["\\class{peachpuff}{#1}", 1], "peru": ["\\class{peru}{#1}", 1], "pink": ["\\class{pink}{#1}", 1], "plum": ["\\class{plum}{#1}", 1], "powderblue": ["\\class{powderblue}{#1}", 1], "rebeccapurple": ["\\class{rebeccapurple}{#1}", 1], "rosybrown": ["\\class{rosybrown}{#1}", 1], "royalblue": ["\\class{royalblue}{#1}", 1], "saddlebrown": ["\\class{saddlebrown}{#1}", 1], "salmon": ["\\class{salmon}{#1}", 1], "sandybrown": ["\\class{sandybrown}{#1}", 1], "seagreen": ["\\class{seagreen}{#1}", 1], "seashell": ["\\class{seashell}{#1}", 1], "sienna": ["\\class{sienna}{#1}", 1], "skyblue": ["\\class{skyblue}{#1}", 1], "slateblue": ["\\class{slateblue}{#1}", 1], "slategray": ["\\class{slategray}{#1}", 1], "slategrey": ["\\class{slategrey}{#1}", 1], "snow": ["\\class{snow}{#1}", 1], "springgreen": ["\\class{springgreen}{#1}", 1], "steelblue": ["\\class{steelblue}{#1}", 1], "tan": ["\\class{tan}{#1}", 1], "thistle": ["\\class{thistle}{#1}", 1], "tomato": ["\\class{tomato}{#1}", 1], "turquoise": ["\\class{turquoise}{#1}", 1], "violet": ["\\class{violet}{#1}", 1], "wheat": ["\\class{wheat}{#1}", 1], "whitesmoke": ["\\class{whitesmoke}{#1}", 1], "yellowgreen": ["\\class{yellowgreen}{#1}", 1]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'capstone-project';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/analytics.js?v=b6170321"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="What is a Pixel and what is an Image?" href="lectures/lecture-1.html" />
    <link rel="prev" title="Website Features" href="website-features.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="July 10, 2025"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt=""/>
    <script>document.write(`<img src="_static/logo_dark.png" class="logo__image only-dark" alt=""/>`);</script>
  
  
    <p class="title logo__title">CSE 4/573 - CVIP</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About the Instructor</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://naresh-ub.github.io">üë®üèª‚Äçüè´ Naresh Kumar Devulapally</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Logistics and Weekly Schedule</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://buffalo.app.box.com/embed/s/agqkobz1pl5dynjiohwchybpnq5ke9py?sortColumn=date">üé• Lecture Recordings</a></li>
<li class="toctree-l1"><a class="reference internal" href="dates-deadlines.html">‚è∞ Dates and Deliverables</a></li>
<li class="toctree-l1"><a class="reference internal" href="syllabus.html">Course Logistics and Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="website-features.html">Website Features</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Capstone Project Details and Deliverables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Course Content</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lectures/lecture-1.html">What is a Pixel and what is an Image?</a></li>
<li class="toctree-l1"><a class="reference internal" href="lectures/lecture-2-3.html">How is an image formed?</a></li>
<li class="toctree-l1"><a class="reference internal" href="lectures/assignment-1.html">üìå Assignment 1 Solution</a></li>


<li class="toctree-l1"><a class="reference internal" href="lectures/lecture-4.html">Image Processing (Part 1) Spatial Domain</a></li>
<li class="toctree-l1"><a class="reference internal" href="lectures/lecture-5.html">Image Processing (Part 2) Fourier Domain</a></li>
<li class="toctree-l1"><a class="reference internal" href="lectures/lecture-6-7-8.html">Depth Estimation using Stereo Vision</a></li>







<li class="toctree-l1"><a class="reference internal" href="lectures/lecture-9.html">Feature Detection and Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="lectures/lecture-10-11.html">Computer Vision applications with Neural Networks</a></li>







<li class="toctree-l1"><a class="reference internal" href="lectures/lecture-12.html">Intro to GenAI: What is a Data Distribution?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/naresh-ub/cvip/blob/main/book/capstone-project.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>



<a href="https://github.com/naresh-ub/cvip" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/capstone-project.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Capstone Project Details and Deliverables</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-deliverables-and-important-deadlines">Summary of deliverables and important deadlines:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-identify-your-category-to-balance-expectations-and-deliverables">Step 1: Identify your category, to balance expectations and deliverables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-choose-a-project-and-form-your-group">Step 2: Choose a project and form your group</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-computer-vision-software-development">Core Computer Vision + Software Development</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-based-models-for-computer-vision-applications">Learning-based Models for Computer Vision applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-ai-projects">Generative AI Projects</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bring-your-own-project-byop">Bring Your Own Project (BYOP)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-go-through-deliverables-and-submit-in-ublearns">Step 3: Go through deliverables and submit in UBLearns</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="capstone-project-details-and-deliverables">
<h1>Capstone Project Details and Deliverables<a class="headerlink" href="#capstone-project-details-and-deliverables" title="Link to this heading">#</a></h1>
<p>CSE 4/573 is the <a class="reference external" href="https://engineering.buffalo.edu/computer-science-engineering/graduate/degrees-and-programs/ms-in-computer-science-and-engineering/ms-tracks-and-specializations.html">Capstone Course</a> at UB. The Capstone Project in this course brings concepts from across the degree into a single, semester-long group project.</p>
<div class="danger admonition">
<p class="admonition-title">Academic Integrity Reminder</p>
<p>Academic integrity is taken very seriously in this course. All submitted work must be your own. Copying code, reports, or results from other students, online sources, or previous course projects will be considered plagiarism and will not be tolerated.</p>
</div>
<section id="summary-of-deliverables-and-important-deadlines">
<h2>Summary of deliverables and important deadlines:<a class="headerlink" href="#summary-of-deliverables-and-important-deadlines" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Project Milestone 1 (Problem Statement Definition, Datasets and Evaluation Metrics Identification).</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">PDF</span> <span class="pre">file</span> <span class="pre">submission.</span></code></p></li>
<li><p>Deadline: July 17, 11:59 PM Eastern on UBLearns.</p></li>
</ul>
</li>
<li><p>Project Milestone 2 (Benchmarking Baselines, Results comparison with a preliminary approach).</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">PDF</span> <span class="pre">file</span> <span class="pre">submission</span> <span class="pre">with</span> <span class="pre">tables</span> <span class="pre">for</span> <span class="pre">results.</span></code></p></li>
<li><p>Deadline: July 31, 11:59 PM Eastern on UBLearns.</p></li>
</ul>
</li>
<li><p>Project Presentations.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">Presentation</span> <span class="pre">file.</span></code></p></li>
<li><p>In-class presentations.</p></li>
<li><p><em>Students volunteer if they want to be in Batch 1 or Batch 2.</em></p></li>
<li><p>Deadline: Batch 1: August 12, Batch 2: August 14, 11:59 PM Eastern.</p></li>
</ul>
</li>
<li><p>Final Project Submission</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">.zip</span> <span class="pre">file</span> <span class="pre">submission</span> <span class="pre">containing:</span> <span class="pre">1</span> <span class="pre">PDF</span> <span class="pre">report,</span> <span class="pre">1</span> <span class="pre">Presentation</span> <span class="pre">file,</span> <span class="pre">Entire</span> <span class="pre">codebase.</span></code></p></li>
<li><p>Deadline: August 14, 11:59 PM Eastern on UBLearns.</p></li>
</ul>
</li>
</ol>
<p>Please check <a class="reference internal" href="dates-deadlines.html"><span class="std std-doc">Dates and Deliverables page</span></a> for more details about the entire course.</p>
<div class="tip admonition">
<p class="admonition-title">Summer 2025 Capstone Course brings a lot of flexibility to students to encourage novel ideas</p>
<p>Dear Students,</p>
<p>As a PhD researcher in Computer Vision, my primary goal for this capstone project is to <strong>encourage creativity, hands-on exploration, and original thinking</strong>.</p>
<p>Instead of assigning a fixed template project, this course gives you the freedom to choose a project that matches your skill level and personal learning goals, whether it‚Äôs your first exposure to computer vision or you‚Äôre preparing for graduate-level research.</p>
<p>To help you get started, I‚Äôve grouped projects into <strong>three categories</strong> so that expectations and deliverables are fair and aligned with your background.</p>
</div>
</section>
<section id="step-1-identify-your-category-to-balance-expectations-and-deliverables">
<h2>Step 1: Identify your category, to balance expectations and deliverables<a class="headerlink" href="#step-1-identify-your-category-to-balance-expectations-and-deliverables" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><span style="background-color:#28a745;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Beginner</span> - <strong>You are new to Computer Vision.</strong><br />
This is for students who are just getting started with image processing or machine learning. You may be an undergraduate or a graduate student taking your first CV course. The goal is to build simple, working applications and gain hands-on exposure to core concepts.</p></li>
<li><p><span style="background-color:orange;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Intermediate</span> - <strong>You have some prior experience with CV and want to go deeper.</strong><br />
You may have worked on a few ML or CV projects and are comfortable using tools like OpenCV, PyTorch, or TensorFlow. The focus is on building moderately complex applications and developing skills for future internships or job roles.</p></li>
<li><p><span style="background-color:red;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Advanced</span> - <strong>Strong expertise in Computer Vision.</strong><br />
You have significant experience with deep learning, academic reading, and model development. Your project should explore a challenging topic with clear benchmarks and may involve experimenting with recent research papers or advanced models.</p></li>
</ol>
<p>Each category has different expectations in terms of project complexity, evaluation, and final deliverables. <em>You are free to choose any project from the list.</em></p>
</section>
<section id="step-2-choose-a-project-and-form-your-group">
<h2>Step 2: Choose a project and form your group<a class="headerlink" href="#step-2-choose-a-project-and-form-your-group" title="Link to this heading">#</a></h2>
<p>Once you‚Äôve identified your category, the next step is to explore the list of curated project ideas. These span across classical vision tasks, deep learning, and generative AI, and are marked by level for easy filtering.</p>
<p>You may work individually or in a group of up to 3 students. Every group must submit a short proposal, choose a project scope appropriate to the category of its members, and begin building.</p>
<p>üìå If you are proposing your own idea, make sure your proposal includes:</p>
<ul class="simple">
<li><p>A clear real-world motivation</p></li>
<li><p>A well-defined problem statement</p></li>
<li><p>Baselines or related work for comparison</p></li>
<li><p>Planned methodology and evaluation metrics</p></li>
</ul>
<section id="core-computer-vision-software-development">
<h3>Core Computer Vision + Software Development<a class="headerlink" href="#core-computer-vision-software-development" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><em>Please keep in mind that the focus is more on functionality rather than aesthetics or frontend.</em></p></li>
<li><p><em>Technologies listed in each project are suggestions only. Students are allowed to choose any frontend/backend frameworks they feel appropriate and comfortable.</em></p></li>
</ul>
<div class="tip dropdown closed admonition">
<p class="admonition-title"><span style="background-color:#28a745;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Beginner</span> Web Application for Basic Computer Vision Tasks</p>
<p>This project is about building a simple web application that helps users understand and apply basic image processing techniques. Users will be able to upload images and see how different image processing operations work, such as converting images to grayscale, detecting edges, and finding key points in the image. The project will also include simple tools for rectifying a pair of images taken from two slightly different angles. <em>Students can extend this project to perform 2D to 3D reconstruction using two images if they are interested.</em></p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Help users understand basic image processing tasks using a visual interface.</p></li>
<li><p>Make it easy to upload and process images in the browser.</p></li>
<li><p>Allow simple camera calibration using chessboard patterns.</p></li>
<li><p>Allow image rectification using feature matching between two uploaded images.</p></li>
<li><p><strong>Features</strong></p>
<ul>
<li><p><strong>Image Upload</strong></p>
<ul>
<li><p>User can upload one or two images using the browser.</p></li>
</ul>
</li>
<li><p><strong>Basic Image Processing</strong></p>
<ul>
<li><p>Convert to grayscale.</p></li>
<li><p>Apply blur to reduce noise.</p></li>
<li><p>Detect edges using the Canny edge detector.</p></li>
<li><p>Adjust image brightness and contrast.</p></li>
</ul>
</li>
<li><p><strong>Feature Detection</strong></p>
<ul>
<li><p>Detect keypoints in an image using ORB (a simple and fast feature detector).</p></li>
<li><p>Match keypoints between two images and draw lines to show matches.</p></li>
</ul>
</li>
<li><p><strong>Camera Calibration (Simple Version)</strong></p>
<ul>
<li><p>User can upload multiple chessboard images.</p></li>
<li><p>The app detects corners and computes the camera calibration matrix.</p></li>
<li><p>Shows the estimated camera matrix and visualizes corner detection.</p></li>
</ul>
</li>
<li><p><strong>Image Rectification</strong></p>
<ul>
<li><p>Given a pair of stereo images, match features and align them.</p></li>
<li><p>Display the aligned (rectified) version of the stereo images side by side.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong> (These are just suggestions, students can use any tool or web technology appropriately)</p>
<ul>
<li><p><strong>Frontend</strong>: HTML, CSS, or any JavaScript library</p></li>
<li><p><strong>Backend</strong>: Python</p></li>
<li><p><strong>Computer Vision</strong>: OpenCV</p></li>
<li><p><strong>Optional</strong>: Docker for containerized deployment</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>A working web application that runs locally or on a server.</p></li>
<li><p>Frontend interface to upload images and view results.</p></li>
<li><p>Python modules:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">processing.py</span></code> for grayscale, blur, edge detection, etc.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">features.py</span></code> for feature detection and matching.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">calibration.py</span></code> for camera calibration functions.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rectify.py</span></code> for stereo rectification logic.</p></li>
</ul>
</li>
<li><p>Clear documentation:</p>
<ul>
<li><p>Setup instructions in <code class="docutils literal notranslate"><span class="pre">README.md</span></code></p></li>
<li><p>Sample test images</p></li>
<li><p>Steps to use each feature</p></li>
</ul>
</li>
<li><p>A sample test suite with example inputs and outputs.</p></li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Learn how to use OpenCV for basic vision tasks.</p></li>
<li><p>Understand how camera calibration and stereo rectification work.</p></li>
<li><p>Practice building a simple full-stack application with Python and web technologies.</p></li>
<li><p>Gain experience in structuring modular, testable code.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="tip dropdown closed admonition">
<p class="admonition-title"><span style="background-color:#28a745;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Beginner</span> Web-Based Lite Image Editing Tool (Lightroom Clone)</p>
<p>This project involves building a simplified version of an image editing application like Adobe Lightroom. The tool will run in the browser and allow users to upload and apply various non-destructive image enhancements. The focus is on basic editing functionality such as exposure, contrast, saturation adjustments, cropping, and applying simple filters. The goal is to help users understand how basic image enhancement and transformation operations work using computer vision techniques.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Provide a visual interface for uploading and editing images in the browser.</p></li>
<li><p>Allow non-destructive edits with real-time preview and adjustable sliders.</p></li>
<li><p>Enable basic image transformations such as rotation and cropping.</p></li>
<li><p>Help users understand how filters and adjustments are applied at the pixel level.</p></li>
</ul>
</li>
<li><p><strong>Features</strong></p>
<ul>
<li><p><strong>Image Upload</strong></p>
<ul>
<li><p>Upload a single image through a simple browser interface.</p></li>
<li><p>Support common image formats such as JPG, PNG, and BMP.</p></li>
</ul>
</li>
<li><p><strong>Adjustment Sliders</strong></p>
<ul>
<li><p>Exposure/Brightness adjustment using scalar multiplication.</p></li>
<li><p>Contrast adjustment using histogram stretching or linear transforms.</p></li>
<li><p>Saturation control for color enhancement.</p></li>
<li><p>Sharpness using kernel filters (e.g., Laplacian).</p></li>
<li><p>Temperature (warm/cool) simulation using color balance.</p></li>
</ul>
</li>
<li><p><strong>Cropping and Rotation</strong></p>
<ul>
<li><p>Basic crop tool using a drag rectangle interface.</p></li>
<li><p>Rotate the image left, right, or by custom angles.</p></li>
</ul>
</li>
<li><p><strong>Preset Filters</strong></p>
<ul>
<li><p>Grayscale</p></li>
<li><p>Sepia</p></li>
<li><p>Vintage</p></li>
<li><p>High contrast black and white</p></li>
</ul>
</li>
<li><p><strong>Preview and Export</strong></p>
<ul>
<li><p>Show live preview of all edits in a canvas.</p></li>
<li><p>Allow users to reset to the original.</p></li>
<li><p>Export the edited image as a downloadable file.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong> (These are just suggestions, students can use any tool or web technology appropriately)</p>
<ul>
<li><p><strong>Frontend</strong>: HTML, CSS, or any JavaScript library</p></li>
<li><p><strong>Backend</strong> (optional): Flask (only if image is processed server-side)</p></li>
<li><p><strong>Image Processing</strong>: OpenCV (if backend), or pure JS (if frontend-only)</p></li>
<li><p><strong>Optional</strong>: Use WebAssembly (WASM) version of OpenCV for in-browser acceleration</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>Fully functional web application with a clean UI.</p></li>
<li><p>Image editor with sliders and toggle buttons for all basic enhancements.</p></li>
<li><p>Optional backend support for saving state or processing larger images.</p></li>
<li><p>Clear modular JavaScript or Python code.</p></li>
<li><p>README documentation with setup and usage instructions.</p></li>
<li><p>A test folder with 3 to 5 sample images to demonstrate functionality.</p></li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Understand basic image enhancement techniques such as brightness and contrast control.</p></li>
<li><p>Learn to work with pixel values directly using JavaScript or OpenCV.</p></li>
<li><p>Learn how to create interactive UI elements like sliders and preview areas.</p></li>
<li><p>Gain confidence in building client-facing image applications from scratch.</p></li>
</ul>
</li>
</ul>
</div>
<div class="none dropdown closed admonition">
<p class="admonition-title"><span style="background-color:orange;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Intermediate</span> Web-Based Object Detection Annotation System</p>
<p>This project is about building a simple web-based annotation tool that allows users to manually annotate bounding boxes for object detection tasks. Users will upload a <code class="docutils literal notranslate"><span class="pre">.zip</span></code> file containing a list of images, and the web interface will allow them to draw bounding boxes, assign class labels, and export the annotations in a standard object detection format. The focus is on creating a clean, easy-to-use tool that can support datasets for training object detection models.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Provide a visual interface for uploading and annotating a batch of images.</p></li>
<li><p>Allow users to draw bounding boxes and assign class labels interactively.</p></li>
<li><p>Export annotations in a format compatible with object detection models (Pascal VOC <code class="docutils literal notranslate"><span class="pre">.xml</span></code> or COCO-style <code class="docutils literal notranslate"><span class="pre">.json</span></code>).</p></li>
<li><p>Keep the tool lightweight, browser-accessible, and beginner-friendly.</p></li>
</ul>
</li>
<li><p><strong>Features</strong></p>
<ul>
<li><p><strong>Image Upload</strong></p>
<ul>
<li><p>Accept a <code class="docutils literal notranslate"><span class="pre">.zip</span></code> file containing images.</p></li>
<li><p>Automatically extract and display image thumbnails for annotation.</p></li>
<li><p>Support common formats like <code class="docutils literal notranslate"><span class="pre">.jpg</span></code>, <code class="docutils literal notranslate"><span class="pre">.jpeg</span></code>, <code class="docutils literal notranslate"><span class="pre">.png</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Annotation Interface</strong></p>
<ul>
<li><p>Display one image at a time for annotation.</p></li>
<li><p>Allow users to:</p>
<ul>
<li><p>Draw multiple bounding boxes.</p></li>
<li><p>Assign class labels to each box.</p></li>
<li><p>Edit or delete boxes before saving.</p></li>
</ul>
</li>
<li><p>Auto-save progress as the user annotates.</p></li>
</ul>
</li>
<li><p><strong>Class Label Input</strong></p>
<ul>
<li><p>Textbox or dropdown to enter/select class label for each box.</p></li>
<li><p>Option to preload a list of class labels from a <code class="docutils literal notranslate"><span class="pre">.txt</span></code> file.</p></li>
</ul>
</li>
<li><p><strong>Navigation</strong></p>
<ul>
<li><p>Buttons to go to next, previous, or specific image.</p></li>
<li><p>Visual indicator showing annotation progress across all images.</p></li>
</ul>
</li>
<li><p><strong>Export Functionality</strong></p>
<ul>
<li><p>Export annotations in <strong>Pascal VOC</strong> XML format (one <code class="docutils literal notranslate"><span class="pre">.xml</span></code> per image).</p></li>
<li><p>Download annotations as a <code class="docutils literal notranslate"><span class="pre">.zip</span></code> of XML files or a consolidated folder.</p></li>
<li><p>Optionally support COCO JSON export in future versions.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong> (These are just suggestions, students can use any tool or web technology appropriately)</p>
<ul>
<li><p><strong>Frontend</strong>: HTML, CSS, or any JavaScript library</p></li>
<li><p><strong>Backend</strong>: Python with Flask or FastAPI (for file upload, extraction, and export packaging)</p></li>
<li><p><strong>Canvas Drawing</strong>: Use HTML5 <code class="docutils literal notranslate"><span class="pre">&lt;canvas&gt;</span></code> API or libraries like <code class="docutils literal notranslate"><span class="pre">fabric.js</span></code> for drawing boxes</p></li>
<li><p><strong>File Handling</strong>: <code class="docutils literal notranslate"><span class="pre">zipfile</span></code>, <code class="docutils literal notranslate"><span class="pre">os</span></code>, and <code class="docutils literal notranslate"><span class="pre">shutil</span></code> for zip extraction and annotation export</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>A fully working web application for local use or deployment on a server.</p></li>
<li><p>Core modules:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">upload_handler.py</span></code>: to handle zip upload and image extraction.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">annotator.js</span></code>: handles drawing logic and frontend interactivity.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exporter.py</span></code>: converts annotation data into Pascal VOC XML files.</p></li>
</ul>
</li>
<li><p>Sample <code class="docutils literal notranslate"><span class="pre">.zip</span></code> of test images and <code class="docutils literal notranslate"><span class="pre">.txt</span></code> of class labels.</p></li>
<li><p>Downloadable output as <code class="docutils literal notranslate"><span class="pre">.zip</span></code> containing <code class="docutils literal notranslate"><span class="pre">.xml</span></code> annotation files.</p></li>
<li><p>A clear README explaining how to use the tool.</p></li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Understand how object detection datasets are annotated and formatted.</p></li>
<li><p>Learn to build user interfaces with drawing capabilities in the browser.</p></li>
<li><p>Practice integrating frontend interaction with backend data export logic.</p></li>
<li><p>Gain hands-on experience with Pascal VOC format and XML generation.</p></li>
</ul>
</li>
</ul>
</div>
<div class="none dropdown closed admonition">
<p class="admonition-title"><span style="background-color:orange;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Intermediate</span> Perspective Correction Tool for Scanned Documents</p>
<p>This project is about building a simple web-based tool to correct the perspective of scanned or photographed documents that are tilted or captured at an angle. Users will upload an image, and the system will detect the document‚Äôs corners and apply a geometric transformation to make the document appear as if it was scanned from a flat, top-down view. If the automatic detection fails, users will be able to manually select the corner points. The result can be downloaded as a cleaned-up <code class="docutils literal notranslate"><span class="pre">.png</span></code> or <code class="docutils literal notranslate"><span class="pre">.pdf</span></code> version of the document.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Automatically correct the perspective of document images.</p></li>
<li><p>Help users clean up images of receipts, forms, or notes taken with a mobile phone.</p></li>
<li><p>Make the correction process fast, user-friendly, and visual.</p></li>
<li><p>Provide manual control for difficult cases.</p></li>
</ul>
</li>
<li><p><strong>Features</strong></p>
<ul>
<li><p><strong>Image Upload</strong></p>
<ul>
<li><p>Upload one image at a time (JPG, PNG).</p></li>
<li><p>Validate file type and display preview.</p></li>
</ul>
</li>
<li><p><strong>Automatic Corner Detection</strong></p>
<ul>
<li><p>Use edge detection and contour approximation to find the document.</p></li>
<li><p>Highlight the detected four corners.</p></li>
<li><p>Display a warning or fallback option if corners cannot be found.</p></li>
</ul>
</li>
<li><p><strong>Manual Corner Adjustment</strong></p>
<ul>
<li><p>Allow users to drag or click on the image to select the four corners.</p></li>
<li><p>Display a zoomed-in preview near the cursor for precision.</p></li>
</ul>
</li>
<li><p><strong>Perspective Correction</strong></p>
<ul>
<li><p>Compute the homography matrix based on the selected corner points.</p></li>
<li><p>Apply the transformation to produce a flat, top-down view of the document.</p></li>
<li><p>Resize or pad output to standard aspect ratio (A4 or original dimensions).</p></li>
</ul>
</li>
<li><p><strong>Export Options</strong></p>
<ul>
<li><p>Download corrected image as <code class="docutils literal notranslate"><span class="pre">.png</span></code>.</p></li>
<li><p>Convert and download the corrected document as a single-page <code class="docutils literal notranslate"><span class="pre">.pdf</span></code>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong> (These are just suggestions, students can use any tool or web technology appropriately)</p>
<ul>
<li><p><strong>Frontend</strong>: HTML, CSS, or any JavaScript library</p></li>
<li><p><strong>Canvas API</strong>: For corner selection and interactive adjustment</p></li>
<li><p><strong>Backend</strong>: Python with Flask (or FastAPI)</p></li>
<li><p><strong>Computer Vision</strong>: OpenCV for edge detection, contour finding, and perspective transformation</p></li>
<li><p><strong>PDF Export</strong>: Python libraries such as <code class="docutils literal notranslate"><span class="pre">reportlab</span></code> or <code class="docutils literal notranslate"><span class="pre">img2pdf</span></code></p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>A clean web application where users can upload and fix document images.</p></li>
<li><p>Automatic and manual workflows for selecting corners.</p></li>
<li><p>Python backend with modules:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">corner_detection.py</span></code> for edge and contour detection.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transform.py</span></code> for homography and warping.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exporter.py</span></code> for image and PDF download.</p></li>
</ul>
</li>
<li><p>Exported results saved locally or offered for download.</p></li>
<li><p>A small set of test images (3‚Äì5 document photos with different angles).</p></li>
<li><p>Documentation:</p>
<ul>
<li><p>README with setup instructions.</p></li>
<li><p>Screenshot examples before and after correction.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Learn how to detect document boundaries using edge and contour methods.</p></li>
<li><p>Understand the mathematics of homography and perspective correction.</p></li>
<li><p>Practice building interactive visual tools for real-world CV use cases.</p></li>
<li><p>Gain experience in converting images into flattened, readable formats.</p></li>
</ul>
</li>
</ul>
</div>
<div class="warning dropdown closed admonition">
<p class="admonition-title"><span style="background-color:red;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Advanced</span> Image Region Labeling Tool for Semantic Segmentation</p>
<p>This project is about building a simple browser-based tool that allows users to annotate regions in an image for semantic segmentation tasks. Users can either draw polygonal masks or paint over regions with a brush tool to define class labels for different areas (e.g., road, sidewalk, car, tree). The tool will generate a pixel-wise labeled mask for each image and export them as <code class="docutils literal notranslate"><span class="pre">.png</span></code> files. A <code class="docutils literal notranslate"><span class="pre">metadata.json</span></code> file will map colors in the mask to class names. This kind of tool is commonly used in preparing datasets for training deep learning models in segmentation tasks.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Allow users to annotate different regions of an image for semantic segmentation.</p></li>
<li><p>Provide tools for both polygon and brush-based annotations.</p></li>
<li><p>Generate accurate, per-pixel labeled masks as output.</p></li>
<li><p>Make the annotation process easy and visual through a web interface.</p></li>
</ul>
</li>
<li><p><strong>Features</strong></p>
<ul>
<li><p><strong>Image Upload</strong></p>
<ul>
<li><p>Upload one or more images to be annotated.</p></li>
<li><p>Support <code class="docutils literal notranslate"><span class="pre">.jpg</span></code>, <code class="docutils literal notranslate"><span class="pre">.jpeg</span></code>, and <code class="docutils literal notranslate"><span class="pre">.png</span></code> formats.</p></li>
<li><p>Display images one by one with navigation.</p></li>
</ul>
</li>
<li><p><strong>Annotation Tools</strong></p>
<ul>
<li><p><strong>Polygon Tool</strong>:</p>
<ul>
<li><p>Click to add points and form a closed shape.</p></li>
<li><p>Fill polygon with a unique color corresponding to a class label.</p></li>
</ul>
</li>
<li><p><strong>Brush Tool</strong>:</p>
<ul>
<li><p>Freehand painting with adjustable brush size.</p></li>
<li><p>Apply color to irregular regions.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Class Label Management</strong></p>
<ul>
<li><p>Add, rename, or delete class labels.</p></li>
<li><p>Assign a unique color to each label.</p></li>
<li><p>Show a class legend beside the canvas for reference.</p></li>
</ul>
</li>
<li><p><strong>Navigation and Progress</strong></p>
<ul>
<li><p>Navigate across multiple uploaded images.</p></li>
<li><p>Show which images have been labeled.</p></li>
<li><p>Option to skip or revisit images.</p></li>
</ul>
</li>
<li><p><strong>Export Annotations</strong></p>
<ul>
<li><p>Save each mask as a <code class="docutils literal notranslate"><span class="pre">.png</span></code> file (same resolution as input).</p></li>
<li><p>Generate a <code class="docutils literal notranslate"><span class="pre">metadata.json</span></code> file that maps RGB color values to class labels.</p></li>
<li><p>Export all files in a single <code class="docutils literal notranslate"><span class="pre">.zip</span></code> for download.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong> (These are just suggestions, students can use any tool or web technology appropriately)</p>
<ul>
<li><p><strong>Frontend</strong>: HTML, CSS, or any JavaScript library</p></li>
<li><p><strong>Canvas API</strong>: For drawing polygons and brush-based painting</p></li>
<li><p><strong>Backend</strong> (optional): Python with Flask (only needed for exporting zip)</p></li>
<li><p><strong>File Packaging</strong>: JavaScript ZIP libraries like <code class="docutils literal notranslate"><span class="pre">JSZip</span></code> or Python <code class="docutils literal notranslate"><span class="pre">zipfile</span></code></p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>A working browser-based tool for image region labeling.</p></li>
<li><p>Two modes of annotation: polygon and brush.</p></li>
<li><p>Class label manager with legend display.</p></li>
<li><p>Export function that produces:</p>
<ul>
<li><p>Per-image <code class="docutils literal notranslate"><span class="pre">.png</span></code> mask with pixel labels.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">metadata.json</span></code> mapping label colors to class names.</p></li>
</ul>
</li>
<li><p>Sample set of 3 to 5 test images for demonstration.</p></li>
<li><p>Documentation:</p>
<ul>
<li><p>README with setup and usage instructions.</p></li>
<li><p>Visual guide for annotating and exporting.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Learn how semantic segmentation datasets are structured and labeled.</p></li>
<li><p>Understand per-pixel annotation formats used in computer vision.</p></li>
<li><p>Gain experience with drawing and user interaction using the HTML5 canvas.</p></li>
<li><p>Learn how to convert user interactions into valid dataset outputs.</p></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="learning-based-models-for-computer-vision-applications">
<h3>Learning-based Models for Computer Vision applications<a class="headerlink" href="#learning-based-models-for-computer-vision-applications" title="Link to this heading">#</a></h3>
<p><em>Students can use Google Colab or Kaggle GPUs for these projects.</em></p>
<div class="tip dropdown closed admonition">
<p class="admonition-title"><span style="background-color:#28a745;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Beginner</span> Monocular Depth Estimation from a Single RGB Image</p>
<p>This project introduces students to the fundamentals of learning-based monocular depth estimation ‚Äî the task of predicting a dense depth map from a single RGB image. The goal is not to achieve state-of-the-art results but to build a simple encoder-decoder depth estimation model from scratch, understand the training pipeline, and visualize predicted depth maps. This project helps students get hands-on experience with supervised regression in computer vision and builds intuition for 2D-to-3D reasoning from images.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Learn how to design and train a neural network that predicts depth from a single RGB image.</p></li>
<li><p>Understand how pixel-level regression differs from classification or segmentation.</p></li>
<li><p>Visualize and interpret model predictions as grayscale or color-coded depth maps.</p></li>
<li><p>Develop an end-to-end training and evaluation pipeline on a standard dataset.</p></li>
</ul>
</li>
<li><p><strong>Features</strong></p>
<ul>
<li><p>Build a fully supervised depth estimation pipeline using PyTorch or TensorFlow.</p></li>
<li><p>Implement a basic CNN-based encoder-decoder architecture.</p></li>
<li><p>Train the network on a subset of a publicly available dataset.</p></li>
<li><p>Normalize, visualize, and save predicted depth maps.</p></li>
<li><p>Compare ground truth and predicted depth maps using error metrics.</p></li>
</ul>
</li>
<li><p><strong>Suggested Architecture</strong></p>
<ul>
<li><p><strong>Encoder</strong>: Use a simple CNN or a pre-trained backbone like ResNet18 (optional).</p></li>
<li><p><strong>Decoder</strong>: Use upsampling layers (e.g., bilinear or transpose convolutions) to produce full-resolution depth maps.</p></li>
<li><p><strong>Loss Function</strong>: L1 loss or Scale-Invariant Log RMSE between predicted and ground truth depths.</p></li>
<li><p><strong>Post-Processing</strong>: Apply normalization for visualization; convert to heatmaps using OpenCV or matplotlib.</p></li>
</ul>
</li>
<li><p><strong>Dataset</strong></p>
<ul>
<li><p><strong>NYU Depth v2 (Indoor Scenes)</strong></p>
<ul>
<li><p>URL: <a class="reference external" href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html</a></p></li>
<li><p>Contains aligned RGB and depth frames from indoor scenes captured with a Kinect sensor.</p></li>
<li><p>Provide a preprocessed subset (~1000 images) to keep training time low.</p></li>
</ul>
</li>
<li><p><strong>KITTI Depth Dataset (Outdoor Driving Scenes)</strong></p>
<ul>
<li><p>URL: <a class="reference external" href="http://www.cvlibs.net/datasets/kitti/">http://www.cvlibs.net/datasets/kitti/</a></p></li>
<li><p>Contains RGB images and sparse LiDAR-based depth maps.</p></li>
<li><p>Recommended only for advanced students due to preprocessing requirements.</p></li>
</ul>
</li>
<li><p><strong>Alternative Starter Datasets</strong></p>
<ul>
<li><p><strong>Make3D</strong>: Smaller, older dataset for quick experimentation.</p></li>
<li><p><strong>Eigen split</strong>: A widely used training split for monocular depth models on KITTI.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong></p>
<ul>
<li><p><strong>Framework</strong>: PyTorch (preferred) or TensorFlow/Keras</p></li>
<li><p><strong>Data Handling</strong>: <code class="docutils literal notranslate"><span class="pre">torchvision</span></code>, custom Dataset classes, image transformations</p></li>
<li><p><strong>Visualization</strong>: matplotlib, OpenCV, or PIL for heatmap overlays</p></li>
<li><p><strong>Training Setup</strong>: GPU-accelerated training (optional), model checkpointing, basic training loop</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>A working training script for monocular depth estimation.</p></li>
<li><p>Model definition (encoder-decoder or UNet-style) in a standalone file.</p></li>
<li><p>Sample visualizations: input RGB, predicted depth map, and ground truth side-by-side.</p></li>
<li><p>Evaluation script to compute RMSE, Abs Rel, and visual difference.</p></li>
<li><p>A README file with:</p>
<ul>
<li><p>Setup instructions and environment dependencies.</p></li>
<li><p>Dataset download and preprocessing steps.</p></li>
<li><p>Training configuration (batch size, epochs, learning rate).</p></li>
<li><p>Explanation of architecture and loss function choices.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Understand the problem of monocular depth estimation and its applications in robotics, AR, and autonomous driving.</p></li>
<li><p>Learn how to design regression-based neural networks for dense prediction tasks.</p></li>
<li><p>Gain experience in handling and visualizing dense depth map data.</p></li>
<li><p>Practice implementing a full supervised training pipeline using modern deep learning frameworks.</p></li>
</ul>
</li>
</ul>
</div>
<div class="tip dropdown closed admonition">
<p class="admonition-title"><span style="background-color:green;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Beginner</span> Object Detection Benchmarking and Analysis</p>
<p>This project focuses on implementing and comparing <strong>three popular object detection models</strong> using a common dataset. The goal is to understand the strengths, limitations, and practical trade-offs of different architectures for object detection. This includes measuring speed, accuracy, and visual outputs across models.</p>
<p>Students will use pretrained models and fine-tune them on a moderately sized dataset to evaluate real-world performance. This project provides hands-on experience in working with bounding boxes, object categories, and COCO-style evaluation.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Learn how modern object detection models work and differ.</p></li>
<li><p>Evaluate and compare multiple models using the same dataset and metrics.</p></li>
<li><p>Understand model trade-offs between accuracy, speed, and complexity.</p></li>
<li><p>Gain experience with annotation formats, evaluation pipelines, and visualization tools.</p></li>
</ul>
</li>
<li><p><strong>Models to Benchmark</strong></p>
<ol class="arabic simple">
<li><p><strong>YOLOv5</strong> (Fast and efficient; good balance of accuracy and speed)</p>
<ul>
<li><p>GitHub: <a class="github reference external" href="https://github.com/ultralytics/yolov5">ultralytics/yolov5</a></p></li>
</ul>
</li>
<li><p><strong>SSD (Single Shot MultiBox Detector)</strong></p>
<ul>
<li><p>PyTorch implementation via <code class="docutils literal notranslate"><span class="pre">torchvision.models.detection.ssd300_vgg16</span></code></p></li>
</ul>
</li>
<li><p><strong>Faster R-CNN</strong> (High accuracy; slower inference)</p>
<ul>
<li><p>PyTorch implementation via <code class="docutils literal notranslate"><span class="pre">torchvision.models.detection.fasterrcnn_resnet50_fpn</span></code></p></li>
</ul>
</li>
</ol>
</li>
<li><p><strong>Dataset</strong></p>
<ul>
<li><p><strong>Pascal VOC 2012</strong></p>
<ul>
<li><p>Public dataset for object detection with 20 classes.</p></li>
<li><p>Contains annotated bounding boxes in XML format.</p></li>
<li><p><a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">VOC 2012 Homepage</a></p></li>
</ul>
</li>
<li><p>Optional: Convert to COCO-style JSON format using tools like <code class="docutils literal notranslate"><span class="pre">voc2coco</span></code> if needed for uniformity.</p></li>
</ul>
</li>
<li><p><strong>Tasks</strong></p>
<ul>
<li><p>Load and prepare dataset, split into training and validation.</p></li>
<li><p>Train or fine-tune each model on the dataset.</p></li>
<li><p>Evaluate using standard object detection metrics:</p>
<ul>
<li><p><strong>mAP&#64;0.5</strong>, <strong>mAP&#64;[0.5:0.95]</strong>, precision, recall</p></li>
<li><p>Inference speed (FPS) and model size</p></li>
</ul>
</li>
<li><p>Visualize predictions from all three models on the same images.</p></li>
<li><p>Log metrics and analyze results.</p></li>
</ul>
</li>
<li><p><strong>Technologies</strong></p>
<ul>
<li><p><strong>Framework</strong>: PyTorch</p></li>
<li><p><strong>Libraries</strong>: OpenCV, Matplotlib, TorchVision</p></li>
<li><p><strong>Annotation Tools</strong>: LabelImg (for manual annotation or correction)</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>A <code class="docutils literal notranslate"><span class="pre">benchmarking/</span></code> folder containing:</p>
<ul>
<li><p>Scripts for training and evaluation of each model.</p></li>
<li><p>Model checkpoints and logs.</p></li>
<li><p>Evaluation plots comparing accuracy, inference speed, and qualitative results.</p></li>
</ul>
</li>
<li><p>A report or Jupyter notebook (<code class="docutils literal notranslate"><span class="pre">detection_analysis.ipynb</span></code>) that includes:</p>
<ul>
<li><p>Overview of each model and architecture.</p></li>
<li><p>Training setup and hyperparameters.</p></li>
<li><p>Evaluation metrics across models.</p></li>
<li><p>Visual comparison of detections (side-by-side bounding boxes).</p></li>
<li><p>Reflections on model performance and trade-offs.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Gain practical experience working with bounding box detection.</p></li>
<li><p>Understand how detection architectures compare in real-world settings.</p></li>
<li><p>Learn to use and modify pretrained models for new datasets.</p></li>
<li><p>Develop analysis and reporting skills for model comparison.</p></li>
</ul>
</li>
<li><p><strong>Optional Extensions</strong></p>
<ul>
<li><p>Test on a custom dataset (e.g., classroom object detection, product shelf detection).</p></li>
<li><p>Add a fourth model (e.g., DETR or YOLOv8).</p></li>
<li><p>Include mobile deployment using ONNX or TensorRT.</p></li>
</ul>
</li>
</ul>
</div>
<div class="none dropdown closed admonition">
<p class="admonition-title"><span style="background-color:orange;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Intermediate</span> Unpaired Day-to-Night Image Translation</p>
<p>This project is about learning to perform <strong>unpaired image-to-image translation</strong>. The goal is to translate daytime street images into realistic nighttime scenes and vice versa, even when the dataset contains no paired examples. Students will train a model (hint: cycleGAN) to perform this style transformation and explore how adversarial losses and cycle consistency enable learning without paired supervision. This project introduces key concepts in generative modeling, domain adaptation, and style transfer.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Understand the concept of unpaired image-to-image translation.</p></li>
<li><p>Implement or fine-tune a model to perform domain transfer between day and night images.</p></li>
<li><p>Visualize and evaluate the quality of generated images.</p></li>
<li><p>Learn how adversarial training and cycle consistency work in practice.</p></li>
</ul>
</li>
<li><p><strong>Features</strong></p>
<ul>
<li><p>Load and preprocess images from two domains: Day and Night.</p></li>
<li><p>Train a model to learn two mappings: Day ‚Üí Night and Night ‚Üí Day.</p></li>
<li><p>Use cycle-consistency loss to ensure that an image translated from one domain and back remains similar to the original.</p></li>
<li><p>Generate side-by-side comparisons of real and translated images.</p></li>
<li><p>Evaluate generated images qualitatively and with simple perceptual metrics.</p></li>
</ul>
</li>
<li><p><strong>Dataset</strong></p>
<ul>
<li><p><strong>BDD100K (Berkeley DeepDrive 100K)</strong></p>
<ul>
<li><p>URL: <a class="reference external" href="https://bdd-data.berkeley.edu">https://bdd-data.berkeley.edu</a></p></li>
<li><p>Use only the image data for this project (no annotations needed).</p></li>
<li><p>Contains over 100,000 street-view driving images with diverse conditions.</p></li>
<li><p>Filter images by <code class="docutils literal notranslate"><span class="pre">timeofday=daytime</span></code> and <code class="docutils literal notranslate"><span class="pre">timeofday=night</span></code> for two separate domains.</p></li>
</ul>
</li>
<li><p><strong>Alternative (if needed)</strong></p>
<ul>
<li><p>Use custom datasets with clear visual distinction (e.g., Google Street View, webcams).</p></li>
<li><p>Minimum: 500+ images per domain, resized to 256√ó256.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong></p>
<ul>
<li><p><strong>Framework</strong>: PyTorch (preferred) or TensorFlow</p></li>
<li><p><strong>Model Architecture</strong>: CycleGAN (generator + discriminator for each domain)</p></li>
<li><p><strong>Image Processing</strong>: torchvision transforms, PIL</p></li>
<li><p><strong>Visualization</strong>: matplotlib, OpenCV</p></li>
</ul>
</li>
<li><p><strong>Key Components</strong></p>
<ul>
<li><p><strong>Generators</strong>: Two U-Net or ResNet-based generators (G: Day ‚Üí Night, F: Night ‚Üí Day)</p></li>
<li><p><strong>Discriminators</strong>: PatchGAN discriminators (D_A and D_B) for adversarial training</p></li>
<li><p><strong>Losses</strong>:</p>
<ul>
<li><p>Adversarial Loss: to make translated images realistic</p></li>
<li><p>Cycle-Consistency Loss: to preserve structure (A ‚Üí B ‚Üí A ‚âà A)</p></li>
<li><p>Identity Loss (optional): to preserve color/style when image already matches target domain</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>A working CycleGAN training pipeline.</p></li>
<li><p>A small subset of day and night images prepared for training.</p></li>
<li><p>Trained models for Day‚ÜíNight and Night‚ÜíDay translation.</p></li>
<li><p>Sample outputs:</p>
<ul>
<li><p>Day ‚Üí Generated Night</p></li>
<li><p>Night ‚Üí Generated Day</p></li>
<li><p>Original vs Reconstructed (Cycle consistency check)</p></li>
</ul>
</li>
<li><p>Evaluation script for saving results and comparing visually.</p></li>
<li><p>README with:</p>
<ul>
<li><p>Setup instructions</p></li>
<li><p>Dataset download and filtering guide</p></li>
<li><p>Training instructions</p></li>
<li><p>Example results</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Learn how GANs work for unpaired translation tasks.</p></li>
<li><p>Understand the role of cycle-consistency and adversarial losses.</p></li>
<li><p>Explore domain adaptation and style transfer in real-world scenes.</p></li>
<li><p>Gain hands-on experience with training large models on real datasets.</p></li>
</ul>
</li>
</ul>
</div>
<div class="none dropdown closed admonition">
<p class="admonition-title"><span style="background-color:orange;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Intermediate</span> Automatic Image Colorization Using CNNs</p>
<p>This project focuses on the task of automatic image colorization, where a model learns to generate plausible colors for grayscale images. The approach involves training a convolutional neural network to predict the <strong>ab color channels</strong> from the <strong>L channel</strong> of an image in the CIELAB color space. Unlike simple filters, this model must learn context and semantics to infer realistic colors (e.g., sky is usually blue, grass is green). The final goal is to produce colorized outputs that look natural and consistent, even though the model is only trained on grayscale inputs.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Learn how to build and train a CNN that maps grayscale (L channel) images to color channels (ab).</p></li>
<li><p>Understand the use of perceptual color spaces (Lab vs RGB) for regression tasks.</p></li>
<li><p>Evaluate the quality of generated images both visually and with perceptual metrics.</p></li>
<li><p>Gain practical experience with pixel-wise regression and image synthesis.</p></li>
</ul>
</li>
<li><p><strong>Features</strong></p>
<ul>
<li><p>Convert RGB images to Lab color space and extract only the L (grayscale) channel.</p></li>
<li><p>Train a CNN to predict ab channels from the L channel using supervised learning.</p></li>
<li><p>Concatenate predicted ab channels with L to reconstruct a full-color image.</p></li>
<li><p>Convert back from Lab to RGB for visualization and evaluation.</p></li>
<li><p>Compare generated images to ground truth RGB and compute quantitative metrics.</p></li>
</ul>
</li>
<li><p><strong>Dataset</strong></p>
<ul>
<li><p><strong>Places365 (Recommended)</strong></p>
<ul>
<li><p>URL: <a class="reference external" href="http://places2.csail.mit.edu/">http://places2.csail.mit.edu/</a></p></li>
<li><p>Large-scale dataset with natural scene diversity (ideal for color learning).</p></li>
<li><p>Resize to 256√ó256 and use a subset (~10k‚Äì50k images) for training.</p></li>
</ul>
</li>
<li><p><strong>Alternative Datasets</strong></p>
<ul>
<li><p>CIFAR-10 (small, good for quick experiments)</p></li>
<li><p>ImageNet subset (for more advanced results)</p></li>
<li><p>COCO images (for object-rich, diverse color contexts)</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong></p>
<ul>
<li><p><strong>Framework</strong>: PyTorch (preferred) or TensorFlow</p></li>
<li><p><strong>Model Architecture</strong>:</p>
<ul>
<li><p>U-Net style encoder-decoder</p></li>
<li><p>Or a custom CNN with downsampling and upsampling blocks</p></li>
</ul>
</li>
<li><p><strong>Color Space Conversions</strong>: <code class="docutils literal notranslate"><span class="pre">skimage.color.rgb2lab</span></code>, <code class="docutils literal notranslate"><span class="pre">lab2rgb</span></code></p></li>
<li><p><strong>Loss Function</strong>: Smooth L1 or MSE loss between predicted and ground truth ab channels</p></li>
</ul>
</li>
<li><p><strong>Key Components</strong></p>
<ul>
<li><p><strong>DataLoader</strong>: Convert RGB images to Lab, feed only the L channel as input</p></li>
<li><p><strong>Model</strong>: Encoder-decoder CNN that takes L and predicts ab channels</p></li>
<li><p><strong>Training Loop</strong>: Feed forward, compute loss on ab, backpropagate</p></li>
<li><p><strong>Postprocessing</strong>: Reconstruct RGB image from predicted ab + original L</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>A complete training pipeline for the CNN-based colorization model</p></li>
<li><p>Model architecture file (e.g., <code class="docutils literal notranslate"><span class="pre">colorization_net.py</span></code>)</p></li>
<li><p>Evaluation script for:</p>
<ul>
<li><p>Side-by-side comparison of input grayscale, predicted color, and ground truth</p></li>
<li><p>Metrics like PSNR, SSIM, LPIPS (optional)</p></li>
</ul>
</li>
<li><p>A set of generated examples saved to disk</p></li>
<li><p>A README with:</p>
<ul>
<li><p>Instructions for setting up the environment and dependencies</p></li>
<li><p>Dataset download and preprocessing instructions</p></li>
<li><p>Training and inference commands</p></li>
<li><p>Notes on how the Lab color space is used</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Understand the problem of pixel-wise regression and why Lab is used for color prediction.</p></li>
<li><p>Learn how to design convolutional architectures for generative vision tasks.</p></li>
<li><p>Develop intuition for training and evaluating generative models that synthesize realistic content.</p></li>
<li><p>Gain confidence in manipulating image formats, color spaces, and visualizations in a deep learning workflow.</p></li>
</ul>
</li>
</ul>
</div>
<div class="warning dropdown closed admonition">
<p class="admonition-title"><span style="background-color:red;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Advanced</span> Image Super-Resolution Using CNNs (SRCNN/ESPCN)</p>
<p>This project focuses on the task of <strong>image super-resolution</strong>, which involves reconstructing a high-resolution image from a low-resolution input. Super-resolution is important in fields like medical imaging, satellite imaging, video enhancement, and forensics. Students will implement a supervised learning pipeline using a simple CNN-based model like SRCNN or ESPCN. The goal is to learn how deep learning can restore high-frequency visual details lost during downsampling.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Learn how to build and train a convolutional neural network for super-resolution.</p></li>
<li><p>Understand the principles of upscaling using deep learning versus traditional interpolation.</p></li>
<li><p>Explore evaluation metrics that measure perceptual image quality.</p></li>
<li><p>Generate side-by-side comparisons of low-resolution, upscaled, and ground truth images.</p></li>
</ul>
</li>
<li><p><strong>Features</strong></p>
<ul>
<li><p>Downsample high-resolution images to create training pairs (LR ‚Üí HR).</p></li>
<li><p>Build a baseline CNN model (SRCNN or ESPCN) for upsampling images.</p></li>
<li><p>Train the model to minimize pixel-wise difference between predicted and true high-resolution images.</p></li>
<li><p>Visualize and save comparison grids of:</p>
<ul>
<li><p>Low-resolution input</p></li>
<li><p>Bicubic interpolation baseline</p></li>
<li><p>Super-resolved output</p></li>
<li><p>Ground truth high-resolution image</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Suggested Architectures</strong></p>
<ul>
<li><p><strong>SRCNN (Super-Resolution CNN)</strong></p>
<ul>
<li><p>A 3-layer CNN that upsamples input using bicubic interpolation followed by refinement.</p></li>
</ul>
</li>
<li><p><strong>ESPCN (Efficient Sub-Pixel CNN)</strong></p>
<ul>
<li><p>Upsamples images using a learned sub-pixel convolution layer (pixel shuffle).</p></li>
</ul>
</li>
<li><p>Optional: Try deeper models like VDSR or SwinIR (if time and GPU allow)</p></li>
</ul>
</li>
<li><p><strong>Dataset</strong></p>
<ul>
<li><p><strong>DIV2K</strong></p>
<ul>
<li><p>URL: <a class="reference external" href="https://data.vision.ee.ethz.ch/cvl/DIV2K/">https://data.vision.ee.ethz.ch/cvl/DIV2K/</a></p></li>
<li><p>High-quality 2K resolution images for super-resolution benchmarking.</p></li>
<li><p>Use the first 800 images for training, 100 for validation.</p></li>
<li><p>Common upscaling factors: √ó2, √ó3, √ó4</p></li>
</ul>
</li>
<li><p><strong>CelebA (Face Images)</strong></p>
<ul>
<li><p>URL: <a class="reference external" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</a></p></li>
<li><p>Center-cropped face dataset, good for domain-specific SR.</p></li>
<li><p>Resize and downsample to generate pairs.</p></li>
</ul>
</li>
<li><p><strong>Custom Dataset (Optional)</strong></p>
<ul>
<li><p>Students can create their own dataset by downsampling any high-quality image folder.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong></p>
<ul>
<li><p><strong>Framework</strong>: PyTorch (preferred) or TensorFlow/Keras</p></li>
<li><p><strong>Loss Function</strong>: L1 or MSE loss on pixel values</p></li>
<li><p><strong>Upsampling Techniques</strong>: Bicubic (for comparison), learned upsampling with <code class="docutils literal notranslate"><span class="pre">torch.nn.PixelShuffle</span></code></p></li>
<li><p><strong>Visualization</strong>: matplotlib, OpenCV</p></li>
</ul>
</li>
<li><p><strong>Evaluation Metrics</strong></p>
<ul>
<li><p><strong>PSNR (Peak Signal-to-Noise Ratio)</strong>: Measures overall pixel-level fidelity.</p></li>
<li><p><strong>SSIM (Structural Similarity Index)</strong>: Measures perceptual similarity and structural quality.</p></li>
<li><p>Optional: <strong>LPIPS</strong> for learning-based perceptual distance (if interested in realism)</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>A complete training pipeline for a super-resolution CNN.</p></li>
<li><p>Image preprocessing scripts for generating LR‚ÄìHR training pairs.</p></li>
<li><p>Inference script that takes a low-resolution image and produces a high-resolution output.</p></li>
<li><p>Side-by-side comparison visuals for test images.</p></li>
<li><p>Evaluation results: PSNR/SSIM scores on validation/test sets.</p></li>
<li><p>README with:</p>
<ul>
<li><p>Dataset download instructions</p></li>
<li><p>Training and inference commands</p></li>
<li><p>Notes on model architecture and loss functions</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Understand how CNNs can learn to restore visual detail from low-resolution inputs.</p></li>
<li><p>Explore the difference between traditional upsampling methods and learned upscaling.</p></li>
<li><p>Learn how to measure perceptual image quality using PSNR, SSIM, and visual inspection.</p></li>
<li><p>Gain experience with preprocessing, training, and evaluating image-to-image regression models.</p></li>
</ul>
</li>
</ul>
</div>
<div class="warning dropdown closed admonition">
<p class="admonition-title"><span style="background-color:red;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Advanced</span> Shadow Removal from Real-World Images Using Deep Learning</p>
<p>This project focuses on building a deep learning model to <strong>remove shadows from natural images</strong>. Shadows are often undesirable in computer vision applications such as autonomous driving, document scanning, and photo editing. The goal is to train a neural network to reconstruct the shadow-free version of an image, given a shadowed input. This task is commonly treated as an image-to-image translation problem, where paired datasets contain both shadowed and shadow-free versions of the same scene.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Learn how to build a deep CNN model for shadow removal.</p></li>
<li><p>Understand challenges involved in learning to remove localized, structured visual obstructions.</p></li>
<li><p>Evaluate the perceptual and quantitative quality of de-shadowed results.</p></li>
<li><p>Gain hands-on experience with paired training data and pixel-level reconstruction loss.</p></li>
</ul>
</li>
<li><p><strong>Features</strong></p>
<ul>
<li><p>Train a supervised image-to-image model using shadowed and shadow-free image pairs.</p></li>
<li><p>Predict clean, shadow-free versions of natural images.</p></li>
<li><p>Visualize original, shadow-free ground truth, and model output side-by-side.</p></li>
<li><p>Compare the model‚Äôs output with traditional shadow correction methods.</p></li>
<li><p>Analyze edge cases where shadows are complex or partially occluded.</p></li>
</ul>
</li>
<li><p><strong>Dataset</strong></p>
<ul>
<li><p><strong>ISTD Dataset (Image Shadow Triplets Dataset)</strong></p>
<ul>
<li><p>URL: <a class="github reference external" href="https://github.com/DeepInsight-PCALab/ST-CGAN">DeepInsight-PCALab/ST-CGAN</a></p></li>
<li><p>Contains 1870 triplets: (shadow image, shadow mask, shadow-free image)</p></li>
<li><p>High-quality images of outdoor and indoor scenes with manually aligned masks</p></li>
<li><p>Download includes:</p>
<ul>
<li><p>Shadow images</p></li>
<li><p>Shadow masks (optional: for visualization or multi-task training)</p></li>
<li><p>Shadow-free ground truths</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Alternative Datasets (Optional/Extension)</strong></p>
<ul>
<li><p>SRD (Shadow Removal Dataset): Larger but noisier annotations</p></li>
<li><p>SBU Shadow Dataset (for shadow detection if students want a multi-task setup)</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong></p>
<ul>
<li><p><strong>Framework</strong>: PyTorch (preferred) or TensorFlow</p></li>
<li><p><strong>Model Architecture Options</strong>:</p>
<ul>
<li><p>U-Net or ResNet-based encoder-decoder</p></li>
<li><p>Optional attention modules to improve performance in shadow regions</p></li>
</ul>
</li>
<li><p><strong>Loss Functions</strong>:</p>
<ul>
<li><p>L1 or L2 reconstruction loss between prediction and ground truth</p></li>
<li><p>Perceptual loss (optional) for texture consistency</p></li>
<li><p>Shadow mask-guided weighted loss (optional)</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Evaluation Metrics</strong></p>
<ul>
<li><p><strong>PSNR</strong> (Peak Signal-to-Noise Ratio)</p></li>
<li><p><strong>SSIM</strong> (Structural Similarity Index)</p></li>
<li><p>Optional: LPIPS or FID if using a GAN-based extension</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>A fully working training pipeline for shadow removal</p></li>
<li><p>Model file (e.g., <code class="docutils literal notranslate"><span class="pre">shadow_removal_net.py</span></code>) implementing the architecture</p></li>
<li><p>Training and evaluation scripts</p></li>
<li><p>Sample results:</p>
<ul>
<li><p>Shadowed input</p></li>
<li><p>Model output</p></li>
<li><p>Ground truth shadow-free image</p></li>
</ul>
</li>
<li><p>Evaluation results (PSNR/SSIM tables)</p></li>
<li><p>README including:</p>
<ul>
<li><p>Dataset download link and preprocessing guide</p></li>
<li><p>Instructions for training, inference, and evaluation</p></li>
<li><p>Explanations of chosen architecture and loss functions</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Understand how CNNs can reconstruct missing or altered visual content.</p></li>
<li><p>Learn to design, train, and debug an image-to-image translation network.</p></li>
<li><p>Develop insights into dealing with real-world, localized artifacts in images.</p></li>
<li><p>Build intuition about the tradeoffs between structural accuracy and visual quality.</p></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="generative-ai-projects">
<h3>Generative AI Projects<a class="headerlink" href="#generative-ai-projects" title="Link to this heading">#</a></h3>
<p><em>Students can use Google Colab or Kaggle GPUs for these projects.</em></p>
<div class="tip dropdown closed admonition">
<p class="admonition-title"><span style="background-color:green;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Beginner</span> Benchmarking GenAI Models: GANs, VAEs, and Diffusion Models</p>
<p><em>This can be run on Google Colab or Kaggle.</em></p>
<p>This project aims to provide hands-on experience with three major classes of generative models ‚Äî <strong>Generative Adversarial Networks (GANs)</strong>, <strong>Variational Autoencoders (VAEs)</strong>, and <strong>Diffusion Models</strong>. Students will implement simplified versions of each model, train them on image datasets (e.g., MNIST, CIFAR-10, CelebA), and compare their generated samples, training dynamics, and evaluation metrics.</p>
<p>The goal is not to beat SOTA but to <strong>understand the mechanisms</strong> behind each approach, gain intuition for their strengths and limitations, and benchmark their performance in a consistent setting.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Implement foundational versions of VAE, GAN, and Denoising Diffusion Probabilistic Models (DDPM).</p></li>
<li><p>Compare their ability to model image data from simple to moderately complex datasets.</p></li>
<li><p>Study training behaviors: convergence, stability, sample quality.</p></li>
<li><p>Evaluate outputs using perceptual and statistical metrics.</p></li>
</ul>
</li>
<li><p><strong>Models to Implement</strong></p>
<ul>
<li><p><strong>VAE</strong> (Kingma &amp; Welling)</p>
<ul>
<li><p>Encoder + decoder architecture</p></li>
<li><p>Latent sampling with reparameterization</p></li>
<li><p>Gaussian likelihood and KL-divergence loss</p></li>
</ul>
</li>
<li><p><strong>GAN</strong> (Goodfellow et al.)</p>
<ul>
<li><p>Simple DCGAN architecture</p></li>
<li><p>Generator vs Discriminator adversarial training</p></li>
<li><p>Use non-saturating loss for stability</p></li>
</ul>
</li>
<li><p><strong>DDPM</strong> (Ho et al., 2020)</p>
<ul>
<li><p>Forward noising process (q)</p></li>
<li><p>Reverse denoising process (pŒ∏)</p></li>
<li><p>Start with basic U-Net-based denoising model</p></li>
<li><p>Use fixed variance schedule for simplicity</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Datasets</strong></p>
<ul>
<li><p><strong>MNIST</strong></p>
<ul>
<li><p>Handwritten digits, grayscale 28√ó28</p></li>
<li><p>Good for fast experimentation</p></li>
</ul>
</li>
<li><p><strong>CIFAR-10</strong></p>
<ul>
<li><p>10 object classes, RGB 32√ó32</p></li>
<li><p>More colorful, diverse scenes</p></li>
</ul>
</li>
<li><p><strong>CelebA (Optional/Advanced)</strong></p>
<ul>
<li><p>Human face images, 64√ó64 or 128√ó128</p></li>
<li><p>For testing on structured data</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong></p>
<ul>
<li><p><strong>Framework</strong>: PyTorch (strongly preferred for modularity)</p></li>
<li><p><strong>Architectures</strong>:</p>
<ul>
<li><p>VAE: MLP or CNN-based encoder/decoder</p></li>
<li><p>GAN: DCGAN-style generator/discriminator</p></li>
<li><p>DDPM: Simple U-Net with cosine or linear noise schedule</p></li>
</ul>
</li>
<li><p><strong>Tools</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>, <code class="docutils literal notranslate"><span class="pre">seaborn</span></code> for visualizations</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torchvision.utils.make_grid()</span></code> for grid image outputs</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Evaluation Metrics</strong></p>
<ul>
<li><p><strong>Inception Score (IS)</strong>: For diversity and realism (on CIFAR-10)</p></li>
<li><p><strong>Fr√©chet Inception Distance (FID)</strong>: Distance between generated and real distribution</p></li>
<li><p><strong>ELBO (for VAE)</strong>: Evidence Lower Bound</p></li>
<li><p><strong>Sample Diversity</strong>: Number of unique samples generated</p></li>
<li><p><strong>Qualitative Analysis</strong>: Visual side-by-side comparisons</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>Three implemented models in <code class="docutils literal notranslate"><span class="pre">vae.py</span></code>, <code class="docutils literal notranslate"><span class="pre">gan.py</span></code>, and <code class="docutils literal notranslate"><span class="pre">diffusion.py</span></code></p></li>
<li><p>Dataset-specific training scripts: <code class="docutils literal notranslate"><span class="pre">train_mnist.py</span></code>, <code class="docutils literal notranslate"><span class="pre">train_cifar.py</span></code>, etc.</p></li>
<li><p>Output folders:</p>
<ul>
<li><p>Checkpoints</p></li>
<li><p>Sample images from each model</p></li>
<li><p>Evaluation plots (loss curves, FID score over epochs)</p></li>
</ul>
</li>
<li><p>A comparison notebook (<code class="docutils literal notranslate"><span class="pre">benchmark.ipynb</span></code>) with:</p>
<ul>
<li><p>Training logs</p></li>
<li><p>Generated image grids</p></li>
<li><p>Evaluation summary table</p></li>
</ul>
</li>
<li><p>README with:</p>
<ul>
<li><p>Environment setup</p></li>
<li><p>Instructions for training and benchmarking</p></li>
<li><p>Notes on hyperparameters and model differences</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Gain working knowledge of three generative modeling paradigms.</p></li>
<li><p>Understand the differences between latent variable models (VAEs), adversarial training (GANs), and iterative refinement (Diffusion Models).</p></li>
<li><p>Develop practical skills in evaluating image generation quality.</p></li>
<li><p>Learn how to design and run experiments fairly across models and datasets.</p></li>
</ul>
</li>
</ul>
</div>
<div class="none dropdown closed admonition">
<p class="admonition-title"><span style="background-color:orange;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Intermediate</span> Face Sketch-to-Photo Generation with Pix2Pix</p>
<p>This project focuses on building a <strong>paired image-to-image translation model</strong> that can generate realistic grayscale face photos from input face sketches. The task is framed as a supervised image translation problem, where the model learns to reconstruct real facial appearances based on edge-level sketch inputs. This problem has real-world applications in criminal identification, digital art, and restoration.</p>
<p>The core idea is to use a lightweight <strong>Pix2Pix</strong> architecture ‚Äî a conditional GAN that takes a sketch as input and generates a realistic photo output. Students will train and evaluate their model using the <strong>CUHK Face Sketch Dataset</strong>.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Understand how to perform image-to-image translation with paired supervision.</p></li>
<li><p>Learn to implement conditional GANs with encoder-decoder generators.</p></li>
<li><p>Generate realistic face images from edge sketches.</p></li>
<li><p>Evaluate reconstruction quality both quantitatively and visually.</p></li>
</ul>
</li>
<li><p><strong>Dataset</strong></p>
<ul>
<li><p><strong>CUHK Face Sketch Dataset (CUFS)</strong></p>
<ul>
<li><p>URL: <a class="reference external" href="https://www.ee.cuhk.edu.hk/~xgwang/sketch.html">https://www.ee.cuhk.edu.hk/~xgwang/sketch.html</a></p></li>
<li><p>Contains 606 image pairs: face sketch + corresponding grayscale photo.</p></li>
<li><p>Sketches are artist-drawn from real photos (aligned and cleaned).</p></li>
<li><p>Pre-aligned 250√ó200 images for easy model input.</p></li>
<li><p>No licensing restrictions for educational use.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Model Architecture</strong></p>
<ul>
<li><p><strong>Generator</strong>: U-Net (with skip connections)</p></li>
<li><p><strong>Discriminator</strong>: PatchGAN (classifies image patches instead of whole image)</p></li>
<li><p><strong>Loss Functions</strong>:</p>
<ul>
<li><p>Adversarial Loss (from GAN framework)</p></li>
<li><p>L1 Reconstruction Loss between generated and ground-truth photo</p></li>
<li><p>Optional: Perceptual Loss for sharper outputs (if GPU permits)</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong></p>
<ul>
<li><p><strong>Framework</strong>: PyTorch</p></li>
<li><p><strong>Training Tools</strong>: torchvision, matplotlib, tqdm</p></li>
<li><p><strong>Data Processing</strong>: Resize to 256√ó256, normalize to [‚àí1, 1]</p></li>
<li><p>Optional lightweight training on Google Colab</p></li>
</ul>
</li>
<li><p><strong>Features</strong></p>
<ul>
<li><p>Upload and view face sketches</p></li>
<li><p>Train Pix2Pix model to learn photo reconstruction</p></li>
<li><p>Visualize outputs side-by-side with original sketches and real photos</p></li>
<li><p>Plot training curves: loss vs epoch</p></li>
<li><p>Generate new photo outputs from unseen sketches</p></li>
</ul>
</li>
<li><p><strong>Evaluation Metrics</strong></p>
<ul>
<li><p><strong>L1 / MAE</strong>: Pixel-level reconstruction error</p></li>
<li><p><strong>SSIM</strong>: Structural similarity to ground truth</p></li>
<li><p><strong>PSNR</strong>: Peak Signal-to-Noise Ratio</p></li>
<li><p><strong>Qualitative</strong>: Visual realism of skin, eyes, hair, and facial outline</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>Clean and modular codebase:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">train.py</span></code>: Handles dataset loading, model training, logging</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">models/</span></code>: Contains generator and discriminator implementations</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">datasets/</span></code>: Preprocessing scripts for CUHK data</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">visualize.py</span></code>: Grid visualizations of sketch ‚Üí generated ‚Üí real</p></li>
</ul>
</li>
<li><p>Output folder containing:</p>
<ul>
<li><p>Saved model checkpoints</p></li>
<li><p>Sample predictions on test set</p></li>
</ul>
</li>
<li><p>Evaluation report (<code class="docutils literal notranslate"><span class="pre">results.md</span></code>) with:</p>
<ul>
<li><p>Visual examples</p></li>
<li><p>Metric tables</p></li>
<li><p>Observations about sketch-to-photo quality</p></li>
</ul>
</li>
<li><p>README:</p>
<ul>
<li><p>Dataset instructions</p></li>
<li><p>Training and inference commands</p></li>
<li><p>Hyperparameter tuning suggestions</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Understand how conditional GANs work in supervised settings</p></li>
<li><p>Learn to process and train on paired image datasets</p></li>
<li><p>Develop practical skills in evaluating visual generation tasks</p></li>
<li><p>Build end-to-end pipelines for training, inference, and visualization</p></li>
</ul>
</li>
</ul>
</div>
<div class="none dropdown closed admonition">
<p class="admonition-title"><span style="background-color:orange;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Intermediate</span> Emoji Generation Using VAE or GAN</p>
<p>This project focuses on building a <strong>generative model for creating new emojis</strong> by learning from a small dataset of existing emoji images. Students will implement either a <strong>Variational Autoencoder (VAE)</strong> or a <strong>DCGAN</strong> to learn the underlying distribution of emojis and sample novel, diverse outputs from the learned latent space. This project introduces students to core generative modeling concepts while being computationally lightweight and visually rewarding.</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Implement a simple VAE or GAN to model emoji distributions.</p></li>
<li><p>Learn to generate new, unseen emojis by sampling from the learned latent space.</p></li>
<li><p>Understand differences in latent sampling, reconstruction loss, and adversarial training.</p></li>
<li><p>Gain practical experience in visualizing and debugging generative models.</p></li>
</ul>
</li>
<li><p><strong>Dataset</strong></p>
<ul>
<li><p><strong>Twemoji or EmojiOne (Open-Source Emojis)</strong></p>
<ul>
<li><p><a class="reference external" href="https://github.com/twitter/twemoji">Twemoji GitHub</a></p></li>
<li><p>Manually download or scrape 200‚Äì500 emojis.</p></li>
<li><p>Preprocess to grayscale or RGB and resize to <strong>64√ó64</strong> pixels.</p></li>
<li><p>Organize into a single folder with <code class="docutils literal notranslate"><span class="pre">.png</span></code> images.</p></li>
</ul>
</li>
<li><p><strong>Optional Dataset Tooling</strong></p>
<ul>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">Pillow</span></code> or <code class="docutils literal notranslate"><span class="pre">OpenCV</span></code> to resize and normalize images.</p></li>
<li><p>Convert emojis to grayscale if you want simpler model training.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Model Options</strong></p>
<ul>
<li><p><strong>VAE</strong>:</p>
<ul>
<li><p>Encoder-decoder architecture with reparameterization</p></li>
<li><p>Latent space sampling and L2 reconstruction loss</p></li>
<li><p>Low training instability, interpretable latent space</p></li>
</ul>
</li>
<li><p><strong>DCGAN</strong>:</p>
<ul>
<li><p>Generator-discriminator adversarial setup</p></li>
<li><p>Can create sharper-looking emoji outputs</p></li>
<li><p>More sensitive to hyperparameters (optional for advanced students)</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Technologies</strong></p>
<ul>
<li><p><strong>Framework</strong>: PyTorch or TensorFlow</p></li>
<li><p><strong>Tools</strong>: matplotlib for visualizing generations, torchvision for image grid rendering</p></li>
<li><p><strong>Training Setup</strong>:</p>
<ul>
<li><p>Batch size: 32</p></li>
<li><p>Resolution: 64√ó64</p></li>
<li><p>Epochs: 50‚Äì100 (can train in &lt;30 mins on Colab)</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Features</strong></p>
<ul>
<li><p>Encode emojis into latent space and visualize latent vectors (for VAE).</p></li>
<li><p>Sample random latent points to generate new emoji images.</p></li>
<li><p>Interpolate between two emojis in latent space.</p></li>
<li><p>Visualize reconstructions vs original images.</p></li>
<li><p>Save generated emojis to a grid for creative viewing.</p></li>
</ul>
</li>
<li><p><strong>Evaluation Metrics</strong></p>
<ul>
<li><p><strong>Reconstruction Loss (VAE)</strong>: L2 or Binary Cross Entropy</p></li>
<li><p><strong>Latent Space Continuity</strong>: Interpolation results</p></li>
<li><p><strong>Visual Diversity</strong>: Number of unique-looking samples</p></li>
<li><p><strong>Optional</strong>: t-SNE plot of latent space clusters</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>A working VAE or GAN implementation in <code class="docutils literal notranslate"><span class="pre">emoji_vae.py</span></code> or <code class="docutils literal notranslate"><span class="pre">emoji_gan.py</span></code></p></li>
<li><p>Training script <code class="docutils literal notranslate"><span class="pre">train.py</span></code> with config flags for model type</p></li>
<li><p>Folder with generated emoji samples over training epochs</p></li>
<li><p>Output grids for:</p>
<ul>
<li><p>Random generations</p></li>
<li><p>Interpolations</p></li>
<li><p>Reconstructions (for VAE)</p></li>
</ul>
</li>
<li><p>Notebook (<code class="docutils literal notranslate"><span class="pre">emoji_analysis.ipynb</span></code>) to visualize generations and latent space</p></li>
<li><p>README:</p>
<ul>
<li><p>Dataset preparation steps</p></li>
<li><p>Model architecture diagram</p></li>
<li><p>Training commands</p></li>
<li><p>Sample outputs and how to evaluate them</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Understand how latent space encoding enables creativity and diversity in generation.</p></li>
<li><p>Learn the differences between VAE (likelihood-based) and GAN (adversarial) modeling.</p></li>
<li><p>Get experience training lightweight generative models under constrained compute.</p></li>
<li><p>Build creative generative AI applications from scratch with real-world use cases.</p></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="bring-your-own-project-byop">
<h3>Bring Your Own Project (BYOP)<a class="headerlink" href="#bring-your-own-project-byop" title="Link to this heading">#</a></h3>
<div class="tip dropdown closed admonition">
<p class="admonition-title"><span style="background-color:#007bff;color:white;padding:1px 4px;border-radius:4px;text-decoration:none;">Open-Ended</span> Student Proposed Projects</p>
<p>This track invites students to design their own project from the ground up by identifying a <strong>real-world problem</strong>, formulating a clear <strong>computer vision problem statement</strong>, and proposing a <strong>well-structured methodology</strong> to solve it. The goal is not only to apply course concepts but to simulate the early stages of applied AI research or product development.</p>
<p>This is ideal for students who want to explore ideas beyond predefined datasets and go deeper into topics aligned with their interests (e.g., medical imaging, agriculture, autonomous systems, education, art).</p>
<ul class="simple">
<li><p><strong>Goals</strong></p>
<ul>
<li><p>Encourage originality, creativity, and initiative.</p></li>
<li><p>Practice defining scope, constraints, and success metrics for real problems.</p></li>
<li><p>Gain experience in building end-to-end projects from ideation to implementation.</p></li>
<li><p>Learn to perform baselining, comparison, and evaluation like a research prototype.</p></li>
</ul>
</li>
<li><p><strong>Proposal Requirements</strong>
Students must submit a 1-page written proposal that clearly outlines:</p>
<ol class="arabic simple">
<li><p><strong>Real-World Motivation</strong></p>
<ul>
<li><p>Describe a real-world situation where the problem arises.</p></li>
<li><p>Explain why it matters and who it affects.</p></li>
<li><p>Example: ‚ÄúFarmers struggle to detect early signs of crop disease using just the naked eye, leading to delayed interventions.‚Äù</p></li>
</ul>
</li>
<li><p><strong>Problem Statement</strong></p>
<ul>
<li><p>Frame the motivation into a specific, measurable CV problem.</p></li>
<li><p>Clearly define inputs, outputs, and the expected model behavior.</p></li>
<li><p>Example: ‚ÄúGiven an RGB image of a crop leaf, classify whether the plant is healthy or affected by one of 5 known diseases.‚Äù</p></li>
</ul>
</li>
<li><p><strong>Baseline Benchmarking</strong></p>
<ul>
<li><p>Identify existing methods that solve similar or related problems.</p></li>
<li><p>Describe one or more baseline models that will be used for comparison.</p></li>
<li><p>Provide links to relevant papers, GitHub repos, or dataset leaderboards.</p></li>
</ul>
</li>
<li><p><strong>Dataset and Preprocessing</strong></p>
<ul>
<li><p>Describe what dataset will be used.</p></li>
<li><p>If it‚Äôs not available publicly, describe the plan to collect or annotate it.</p></li>
<li><p>Describe the input format, image resolution, preprocessing, and splits.</p></li>
</ul>
</li>
<li><p><strong>Proposed Method</strong></p>
<ul>
<li><p>High-level architecture (e.g., CNN, UNet, Transformer).</p></li>
<li><p>Justify why the proposed method is expected to work better or differently than the baseline.</p></li>
<li><p>Optional extensions like multi-modal inputs or data augmentations.</p></li>
</ul>
</li>
<li><p><strong>Evaluation Plan</strong></p>
<ul>
<li><p>Define the metrics: accuracy, F1-score, PSNR, SSIM, IoU, etc.</p></li>
<li><p>Include how experiments will be logged, visualized, and interpreted.</p></li>
<li><p>Describe comparison with baselines and ablation studies (if applicable).</p></li>
</ul>
</li>
<li><p><strong>Deliverables</strong></p>
<ul>
<li><p>Working code and trained models.</p></li>
<li><p>README and reproducible instructions.</p></li>
<li><p>Visualizations of results (e.g., confusion matrix, segmentation masks, GAN outputs).</p></li>
<li><p>Report/Notebook summarizing methodology, experiments, and learnings.</p></li>
</ul>
</li>
</ol>
</li>
<li><p><strong>Examples of Past Student Projects</strong></p>
<ul>
<li><p><em>‚ÄúDetecting potholes in road surfaces from dashcam footage‚Äù</em></p></li>
<li><p><em>‚ÄúMonocular 3D object detection in construction site scenes‚Äù</em></p></li>
<li><p><em>‚ÄúFine-grained classification of rock types in geological samples‚Äù</em></p></li>
<li><p><em>‚ÄúGenerating anime avatars from hand-drawn sketches‚Äù</em></p></li>
</ul>
</li>
<li><p><strong>Learning Outcomes</strong></p>
<ul>
<li><p>Learn to independently scope and define a real AI project.</p></li>
<li><p>Develop skills in literature review, baseline benchmarking, and critical analysis.</p></li>
<li><p>Understand the research process: identifying gaps, validating methods, and iterating.</p></li>
<li><p>Practice communicating a vision both technically and narratively.</p></li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<section id="step-3-go-through-deliverables-and-submit-in-ublearns">
<h2>Step 3: Go through deliverables and submit in UBLearns<a class="headerlink" href="#step-3-go-through-deliverables-and-submit-in-ublearns" title="Link to this heading">#</a></h2>
</section>
</section>

        <script type="text/x-thebe-config">
        {
            "rootPath": "./",
            "requestKernel": true,
            "useJupyterLite": true,
            "useBinder": false,
            "kernelOptions": {
                "path": "/"
            },
            "codeMirrorConfig": {
                "theme": "default",
                "mode": "python"
            },
            "mountRestartButton": false,
            "mountRestartallButton": false
        }
        </script>
        <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="website-features.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Website Features</p>
      </div>
    </a>
    <a class="right-next"
       href="lectures/lecture-1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">What is a Pixel and what is an Image?</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-deliverables-and-important-deadlines">Summary of deliverables and important deadlines:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-identify-your-category-to-balance-expectations-and-deliverables">Step 1: Identify your category, to balance expectations and deliverables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-choose-a-project-and-form-your-group">Step 2: Choose a project and form your group</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-computer-vision-software-development">Core Computer Vision + Software Development</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-based-models-for-computer-vision-applications">Learning-based Models for Computer Vision applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-ai-projects">Generative AI Projects</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bring-your-own-project-byop">Bring Your Own Project (BYOP)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-go-through-deliverables-and-submit-in-ublearns">Step 3: Go through deliverables and submit in UBLearns</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Naresh Devulapally, Summer 2025, UB. Built with Teachbooks.
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on July 10, 2025.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>