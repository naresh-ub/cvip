selector_to_html = {"a[href=\"lectures/lecture-10-11.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Computer Vision applications with Neural Networks<a class=\"headerlink\" href=\"#computer-vision-applications-with-neural-networks\" title=\"Link to this heading\">#</a></h1><p>In this section, we transition from classical vision techniques to modern <strong>data-driven learning</strong> methods: neural networks.</p><p>The core idea is simple: a neural network is a <strong>trainable function approximator</strong>. It can learn to represent very complex mappings by adjusting its internal parameters.</p>", "a[href=\"lectures/lecture-12.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Intro to GenAI: What is a Data Distribution?<a class=\"headerlink\" href=\"#intro-to-genai-what-is-a-data-distribution\" title=\"Link to this heading\">#</a></h1><h2>Function Approximators<a class=\"headerlink\" href=\"#function-approximators\" title=\"Link to this heading\">#</a></h2><p>Neural networks are universal function approximators.<br/>\nThey learn to model an unknown function <span class=\"math notranslate nohighlight\">\\(y = f(x)\\)</span> by training on pairs of <span class=\"math notranslate nohighlight\">\\((x, y)\\)</span>.</p><p>Given data points:</p>", "a[href=\"syllabus.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Course Syllabus<a class=\"headerlink\" href=\"#course-syllabus\" title=\"Link to this heading\">#</a></h1><h2>Course Information<a class=\"headerlink\" href=\"#course-information\" title=\"Link to this heading\">#</a></h2>", "a[href=\"lectures/lecture-9.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Feature Detection and Extraction<a class=\"headerlink\" href=\"#feature-detection-and-extraction\" title=\"Link to this heading\">#</a></h1><p>In this chapter, we move from <strong>depth estimation</strong> to one of the core tasks in classical computer vision: <strong>finding good feature points</strong>.</p>", "a[href=\"course-schedule.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Course Schedule<a class=\"headerlink\" href=\"#course-schedule\" title=\"Link to this heading\">#</a></h1><p>This Computer Vision course runs from <strong>May 27 to August 14, 2025</strong>, meeting every Tuesday and Thursday. The curriculum progresses from fundamental concepts (image formation, processing) through core techniques (stereo vision, feature detection) to advanced topics (VAEs, diffusion models).</p><p>Key deliverables include:</p>", "a[href=\"lectures/lecture-13.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Feature Extractors for Generative Models<a class=\"headerlink\" href=\"#feature-extractors-for-generative-models\" title=\"Link to this heading\">#</a></h1><h2>1. Neural Networks as Feature Extractors<a class=\"headerlink\" href=\"#neural-networks-as-feature-extractors\" title=\"Link to this heading\">#</a></h2><p>A <strong>Neural Network (NN)</strong> can be decomposed into two conceptual parts:</p>", "a[href=\"lectures/lecture-15.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Diffusion Models<a class=\"headerlink\" href=\"#diffusion-models\" title=\"Link to this heading\">#</a></h1><h2>1. Motivation and Recap<a class=\"headerlink\" href=\"#motivation-and-recap\" title=\"Link to this heading\">#</a></h2><p><strong>Diffusion Models</strong> offer a new paradigm:</p>", "a[href=\"lectures/lecture-5.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Image Processing (Part 2) Fourier Domain<a class=\"headerlink\" href=\"#image-processing-part-2-fourier-domain\" title=\"Link to this heading\">#</a></h1><h2>What is a Function in the Time Domain?<a class=\"headerlink\" href=\"#what-is-a-function-in-the-time-domain\" title=\"Link to this heading\">#</a></h2><p><strong>Mathematically:</strong><br/>\n<span class=\"math notranslate nohighlight\">\\( f(t): \\mathbb{R} \\to \\mathbb{R} \\)</span> \u2014 maps time to amplitude.</p>", "a[href=\"lectures/lecture-4.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Image Processing (Part 1) Spatial Domain<a class=\"headerlink\" href=\"#image-processing-part-1-spatial-domain\" title=\"Link to this heading\">#</a></h1><p>As we established multiple times in this course that images are nothing but <code class=\"docutils literal notranslate\"><span class=\"pre\">numpy</span> <span class=\"pre\">arrays</span></code> where each value in the grid represents a pixel. In the case of a grayscale image, the pixel represents intensity from <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span> <span class=\"pre\">to</span> <span class=\"pre\">255</span></code> and in the case of a colored RGB image, each pixel is a vector <code class=\"docutils literal notranslate\"><span class=\"pre\">[R,</span> <span class=\"pre\">G,</span> <span class=\"pre\">B]</span></code> values ranging from 0 to 255. Depending on different combinations of R, G, and B, we can visualize different colors at each pixel.</p><p><em>In this lecture, we will look at various image processing techniques that will result in extraction of various features from images.</em></p>", "a[href=\"#about-the-course\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">About the Course<a class=\"headerlink\" href=\"#about-the-course\" title=\"Link to this heading\">#</a></h2><p>CSE 4/573: Computer Vision &amp; Generative AI (Summer 2025) at the University at Buffalo offered students a comprehensive journey from <strong>classical computer vision foundations to cutting-edge generative AI</strong>. The curriculum began with the mathematical and physical principles of image formation, camera calibration, and pixel- and frequency-domain image processing, before progressing to geometric methods such as stereo vision, depth estimation, and epipolar geometry. Building on this foundation, students explored modern learning-based approaches, including convolutional neural networks for classification, segmentation, and object detection. <strong>A hallmark of the course was a dedicated generative vision module, covering variational autoencoders, GANs, and diffusion models</strong>, which placed students at the forefront of contemporary computer vision. By the end of the semester, students had achieved both theoretical mastery and practical skills, developing intelligent vision systems using frameworks like OpenCV, TensorFlow, and PyTorch for real-world applications.</p>", "a[href=\"lectures/lecture-2-3.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">How is an image formed?<a class=\"headerlink\" href=\"#how-is-an-image-formed\" title=\"Link to this heading\">#</a></h1><p>In this lecture, we shall understand the Math and Geometry behind Image Formation. In the <a class=\"reference internal\" href=\"lectures/lecture-1.html\"><span class=\"std std-doc\">previous lecture</span></a>, we learned that images are stored/read as <code class=\"docutils literal notranslate\"><span class=\"pre\">numpy</span> <span class=\"pre\">arrays</span></code>. In this lecture we understand the way in which a 3D object is captured on a 2D image plane. In fact, we will see that a <code class=\"docutils literal notranslate\"><span class=\"pre\">camera</span> <span class=\"pre\">is</span> <span class=\"pre\">nothing</span> <span class=\"pre\">but</span> <span class=\"pre\">a</span> <span class=\"pre\">function</span> <span class=\"pre\">that</span> <span class=\"pre\">maps</span> <span class=\"pre\">a</span> <span class=\"pre\">3D</span> <span class=\"pre\">point</span> <span class=\"pre\">in</span> <span class=\"pre\">real-world</span> <span class=\"pre\">onto</span> <span class=\"pre\">a</span> <span class=\"pre\">2D</span> <span class=\"pre\">point</span> <span class=\"pre\">on</span> <span class=\"pre\">the</span> <span class=\"pre\">image</span> <span class=\"pre\">plane</span></code>.</p><p>Let\u2019s get started with an object that we want to capture. For this lecture, let\u2019s choose an object with regular shape.</p>", "a[href=\"#student-feedback\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">Student Feedback<a class=\"headerlink\" href=\"#student-feedback\" title=\"Link to this heading\">#</a></h2><h3>Feedback welcome!<a class=\"headerlink\" href=\"#feedback-welcome\" title=\"Link to this heading\">#</a></h3><p>If you found this course website useful, please consider the following:</p>", "a[href=\"lectures/lecture-16.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Diffusion Model on Swiss Roll (DDPM) - PyTorch Code<a class=\"headerlink\" href=\"#diffusion-model-on-swiss-roll-ddpm-pytorch-code\" title=\"Link to this heading\">#</a></h1><p>In this notebook, we demonstrate a basic Diffusion Model training loop on a 2D Swiss Roll dataset.</p>", "a[href=\"#feedback-welcome\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">Feedback welcome!<a class=\"headerlink\" href=\"#feedback-welcome\" title=\"Link to this heading\">#</a></h3><p>If you found this course website useful, please consider the following:</p>", "a[href=\"lectures/lecture-6-7-8.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Depth Estimation using Stereo Vision<a class=\"headerlink\" href=\"#depth-estimation-using-stereo-vision\" title=\"Link to this heading\">#</a></h1><h2>What is Stereo Vision?<a class=\"headerlink\" href=\"#what-is-stereo-vision\" title=\"Link to this heading\">#</a></h2><p>Stereo vision is a <strong>classical computer vision technique</strong> that mimics human binocular vision to perceive depth. Just like your left and right eyes capture slightly different views of the world, two cameras placed at slightly different positions observe the same scene from different angles.</p>", "a[href=\"#id1\"]": "<figure class=\"align-default\" id=\"id1\">\n<img alt=\"_images/course_pic2.jpeg\" src=\"_images/course_pic2.jpeg\"/>\n<figcaption>\n<p><span class=\"caption-number\">Fig. 1 </span><span class=\"caption-text\">In-class pictures with a few students (Students Enrolled: 71)</span><a class=\"headerlink\" href=\"#id1\" title=\"Link to this image\">#</a></p>\n</figcaption>\n</figure>", "a[href=\"capstone-project.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Capstone Project Details and Deliverables<a class=\"headerlink\" href=\"#capstone-project-details-and-deliverables\" title=\"Link to this heading\">#</a></h1><p>CSE 4/573 is the <a class=\"reference external\" href=\"https://engineering.buffalo.edu/computer-science-engineering/graduate/degrees-and-programs/ms-in-computer-science-and-engineering/ms-tracks-and-specializations.html\">Capstone Course</a> at UB. The Capstone Project in this course brings concepts from across the degree into a single, semester-long group project.</p>", "a[href=\"#instructor\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">Instructor<a class=\"headerlink\" href=\"#instructor\" title=\"Link to this heading\">#</a></h2>", "a[href=\"#course-design-experience\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">Course Design Experience<a class=\"headerlink\" href=\"#course-design-experience\" title=\"Link to this heading\">#</a></h2><p>As the <code class=\"docutils literal notranslate\"><span class=\"pre\">first</span> <span class=\"pre\">course</span> <span class=\"pre\">at</span> <span class=\"pre\">the</span> <span class=\"pre\">University</span> <span class=\"pre\">at</span> <span class=\"pre\">Buffalo</span> <span class=\"pre\">to</span> <span class=\"pre\">incorporate</span> <span class=\"pre\">diffusion</span> <span class=\"pre\">models</span> <span class=\"pre\">in</span> <span class=\"pre\">a</span> <span class=\"pre\">semester-long</span> <span class=\"pre\">module</span></code>, CSE 4/573 distinguished itself through its emphasis on generative AI. The course materials for VAEs, GANs, and diffusion models were <strong>created entirely from the ground up, blending intuitive mathematical derivations with from-scratch Python implementations</strong>. Assignments were structured to guide students from foundational generative modeling exercises to advanced explorations, culminating in a <a class=\"reference internal\" href=\"capstone-project.html\"><span class=\"doc std std-doc\">capstone project</span></a> that challenged them to integrate generative methods into practical applications such as controllable image synthesis and multimodal vision-language systems. This focus on generative AI ensured that students not only gained early exposure to a rapidly evolving research frontier but also left the course with concrete, portfolio-ready projects demonstrating both technical rigor and creativity.</p>", "a[href=\"#important-links\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">Important Links<a class=\"headerlink\" href=\"#important-links\" title=\"Link to this heading\">#</a></h2>", "a[href=\"website-features.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Website Features<a class=\"headerlink\" href=\"#website-features\" title=\"Link to this heading\">#</a></h1><p><strong>Welcome</strong> to the interactive side!</p><p>The Summer 2025 version of this course has been completely revamped on two key fundamentals:</p>", "a[href=\"#cse-4-573-computer-vision-generative-ai\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">CSE 4/573: Computer Vision &amp; Generative AI<a class=\"headerlink\" href=\"#cse-4-573-computer-vision-generative-ai\" title=\"Link to this heading\">#</a></h1><h2>About the Course<a class=\"headerlink\" href=\"#about-the-course\" title=\"Link to this heading\">#</a></h2><p>CSE 4/573: Computer Vision &amp; Generative AI (Summer 2025) at the University at Buffalo offered students a comprehensive journey from <strong>classical computer vision foundations to cutting-edge generative AI</strong>. The curriculum began with the mathematical and physical principles of image formation, camera calibration, and pixel- and frequency-domain image processing, before progressing to geometric methods such as stereo vision, depth estimation, and epipolar geometry. Building on this foundation, students explored modern learning-based approaches, including convolutional neural networks for classification, segmentation, and object detection. <strong>A hallmark of the course was a dedicated generative vision module, covering variational autoencoders, GANs, and diffusion models</strong>, which placed students at the forefront of contemporary computer vision. By the end of the semester, students had achieved both theoretical mastery and practical skills, developing intelligent vision systems using frameworks like OpenCV, TensorFlow, and PyTorch for real-world applications.</p>", "a[href=\"#teaching-philosophy-and-approach\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">Teaching Philosophy and Approach<a class=\"headerlink\" href=\"#teaching-philosophy-and-approach\" title=\"Link to this heading\">#</a></h2><p>My (<a class=\"reference external\" href=\"https://naresh-ub.github.io\">Naresh Devulapally</a>) teaching approach is rooted in <strong>intuition and curiosity</strong>. To make abstract concepts tangible, I crafted every slide from scratch with rich <strong>visuals, animations, and interactive plots</strong>, reinforcing learning through quizzes and flashcards that promote long-term retention. Curiosity was cultivated through <strong>interactive live coding on any browser</strong> directly , where each algorithm was implemented interactively from first principles in Python, <strong>without requiring students to set up local environments</strong>. To support this, I developed a comprehensive course website powered by <a class=\"reference external\" href=\"https://jupyterbook.org/\">Jupyter Book</a> and TeachBooks, hosting animated slides via <a class=\"reference external\" href=\"https://revealjs.com/\">Reveal.JS</a>, executable code blocks with <a class=\"reference external\" href=\"https://github.com/executablebooks/thebe\">Thebe</a>, 3D visualizations using <a class=\"reference external\" href=\"https://plotly.com/python/\">Plotly</a>, spaced-repetition flashcards through <a class=\"reference external\" href=\"https://github.com/jmshea/jupytercards\">JupyterCards</a>, and auto-graded practice quizzes. This ecosystem transformed the course into an engaging, hands-on experience that blended rigorous theory with immediate experimentation, equipping students to both understand and innovate in Computer Vision.</p>", "a[href=\"lectures/lecture-14.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Conditional VAEs, GANs and Conditional GANs<a class=\"headerlink\" href=\"#conditional-vaes-gans-and-conditional-gans\" title=\"Link to this heading\">#</a></h1><h2>1. Conditional Variational Autoencoders (CVAEs)<a class=\"headerlink\" href=\"#conditional-variational-autoencoders-cvaes\" title=\"Link to this heading\">#</a></h2><h3>1.1 Motivation<a class=\"headerlink\" href=\"#motivation\" title=\"Link to this heading\">#</a></h3><p>The standard VAE generates data from an unstructured latent space. But often we want <strong>controlled generation</strong> based on side information (e.g., labels, class, attributes).</p><p>This leads to <strong>Conditional VAEs</strong>, where generation is conditioned on <span class=\"math notranslate nohighlight\">\\( \\mathbf{y} \\)</span> (a label or auxiliary variable).</p>", "a[href=\"lectures/lecture-1.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">What is a Pixel and what is an Image?<a class=\"headerlink\" href=\"#what-is-a-pixel-and-what-is-an-image\" title=\"Link to this heading\">#</a></h1><p>Images are made up of <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Pixel#:~:text=In%20digital%20imaging%2C%20a%20pixel,can%20be%20manipulated%20through%20software\">pixels</a>, small units that represent the intensity or color at specific coordinates. Each pixel can be a single grayscale value or a tuple of values for color channels like RGB. In scientific computing and computer vision, images are typically stored as NumPy arrays: 2D arrays for grayscale images and 3D arrays (height \u00d7 width \u00d7 channels) for color images. This structured representation allows efficient numerical manipulation, making NumPy a powerful foundation for image processing, filtering, and machine learning applications.</p>", "a[href=\"lectures/lecture-17.html\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Conditional Diffusion Models<a class=\"headerlink\" href=\"#conditional-diffusion-models\" title=\"Link to this heading\">#</a></h1><h2>1. Conditional Diffusion Models<a class=\"headerlink\" href=\"#id1\" title=\"Link to this heading\">#</a></h2><p>Standard diffusion models sample from an unconditional distribution <span class=\"math notranslate nohighlight\">\\( p(\\mathbf{x}) \\)</span>.<br/>\nTo control the generation process, we condition on auxiliary information <span class=\"math notranslate nohighlight\">\\( \\mathbf{y} \\)</span> (e.g., class labels, text prompts, segmentation masks).</p><p>The conditional generative process is:</p>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(`main ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: false,
                placement: 'auto-start', maxWidth: 500, interactive: true, boundary: document.body, appendTo: document.body,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
