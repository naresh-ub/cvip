selector_to_html = {"a[href=\"#autoencoders-aes\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">3. Autoencoders (AEs)<a class=\"headerlink\" href=\"#autoencoders-aes\" title=\"Link to this heading\">#</a></h2><p>An <strong>Autoencoder</strong> is a special neural network trained to reconstruct its input.<br/>\nIt consists of two main parts:</p>", "a[href=\"#feature-extractors-for-generative-models\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Feature Extractors for Generative Models<a class=\"headerlink\" href=\"#feature-extractors-for-generative-models\" title=\"Link to this heading\">#</a></h1><h2>1. Neural Networks as Feature Extractors<a class=\"headerlink\" href=\"#neural-networks-as-feature-extractors\" title=\"Link to this heading\">#</a></h2><p>A <strong>Neural Network (NN)</strong> can be decomposed into two conceptual parts:</p>", "a[href=\"#limitations-of-standard-autoencoders\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">4. Limitations of Standard Autoencoders<a class=\"headerlink\" href=\"#limitations-of-standard-autoencoders\" title=\"Link to this heading\">#</a></h2><p>This motivates <strong>Variational Autoencoders</strong>.</p>", "a[href=\"#variational-inference\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">5.2 Variational Inference<a class=\"headerlink\" href=\"#variational-inference\" title=\"Link to this heading\">#</a></h3><p>Introduce an <strong>approximate posterior</strong> <span class=\"math notranslate nohighlight\">\\( q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\)</span> (the encoder).</p><p>We optimize the <strong>Evidence Lower Bound (ELBO):</strong></p>", "a[href=\"#reparameterization-trick\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">5.3 Reparameterization Trick<a class=\"headerlink\" href=\"#reparameterization-trick\" title=\"Link to this heading\">#</a></h3><p>To backpropagate through stochastic sampling, we reparameterize:</p>", "a[href=\"#inference-with-aes-and-vaes\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">6. Inference with AEs and VAEs<a class=\"headerlink\" href=\"#inference-with-aes-and-vaes\" title=\"Link to this heading\">#</a></h2>", "a[href=\"#simple-mlp-autoencoder\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">3.1 Simple MLP Autoencoder<a class=\"headerlink\" href=\"#simple-mlp-autoencoder\" title=\"Link to this heading\">#</a></h3>", "a[href=\"#variational-autoencoders-vaes\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">5. Variational Autoencoders (VAEs)<a class=\"headerlink\" href=\"#variational-autoencoders-vaes\" title=\"Link to this heading\">#</a></h2><h3>5.1 Generative Modeling<a class=\"headerlink\" href=\"#generative-modeling\" title=\"Link to this heading\">#</a></h3><p>We assume data <span class=\"math notranslate nohighlight\">\\( \\mathbf{x} \\)</span> is generated from latent variables <span class=\"math notranslate nohighlight\">\\( \\mathbf{z} \\)</span> via:</p>", "a[href=\"#neural-networks-as-feature-extractors\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">1. Neural Networks as Feature Extractors<a class=\"headerlink\" href=\"#neural-networks-as-feature-extractors\" title=\"Link to this heading\">#</a></h2><p>A <strong>Neural Network (NN)</strong> can be decomposed into two conceptual parts:</p>", "a[href=\"#convolutional-autoencoder\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">3.2 Convolutional Autoencoder<a class=\"headerlink\" href=\"#convolutional-autoencoder\" title=\"Link to this heading\">#</a></h3><p>As the autoencoder above reduces the features to <strong>two dimensions</strong>, we can easily visualize the features. The plot above provides us with a convenient way to generate images from 2D features.</p>", "a[href=\"#generative-modeling\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">5.1 Generative Modeling<a class=\"headerlink\" href=\"#generative-modeling\" title=\"Link to this heading\">#</a></h3><p>We assume data <span class=\"math notranslate nohighlight\">\\( \\mathbf{x} \\)</span> is generated from latent variables <span class=\"math notranslate nohighlight\">\\( \\mathbf{z} \\)</span> via:</p>", "a[href=\"#summary\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">7. Summary<a class=\"headerlink\" href=\"#summary\" title=\"Link to this heading\">#</a></h2>", "a[href=\"#feature-space-and-low-dimensional-manifolds\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">2. Feature Space and Low-Dimensional Manifolds<a class=\"headerlink\" href=\"#feature-space-and-low-dimensional-manifolds\" title=\"Link to this heading\">#</a></h2><p>Consider images <span class=\"math notranslate nohighlight\">\\( \\mathbf{x}_i \\)</span> and <span class=\"math notranslate nohighlight\">\\( \\mathbf{x}_j \\)</span>. If they are \u201ccloser\u201d in visual content, their <strong>feature representations</strong> <span class=\"math notranslate nohighlight\">\\( f_{\\theta}(\\mathbf{x}_i) \\)</span> and <span class=\"math notranslate nohighlight\">\\( f_{\\theta}(\\mathbf{x}_j) \\)</span> will lie close in the learned low-dimensional manifold.</p><p>This property enables <strong>clustering, visualization, and transfer learning</strong>.</p>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(`main ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: false,
                placement: 'auto-start', maxWidth: 500, interactive: true, boundary: document.body, appendTo: document.body,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
