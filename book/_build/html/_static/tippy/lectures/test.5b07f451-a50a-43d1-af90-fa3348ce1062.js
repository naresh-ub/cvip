selector_to_html = {"a[href=\"#universal-components-across-most-neural-networks\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">3. Universal components across (most) neural networks<a class=\"headerlink\" href=\"#universal-components-across-most-neural-networks\" title=\"Link to this heading\">#</a></h2><h3>3.1 Problem statement &amp; Empirical Risk Minimization (ERM)<a class=\"headerlink\" href=\"#problem-statement-empirical-risk-minimization-erm\" title=\"Link to this heading\">#</a></h3><p>Given a parametric model <span class=\"math notranslate nohighlight\">\\(f_\\theta\\)</span> and loss <span class=\"math notranslate nohighlight\">\\(\\ell(\\cdot,\\cdot)\\)</span>, we minimize the <strong>empirical risk</strong>:</p>", "a[href=\"#loss-functions\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">3.2 Loss functions<a class=\"headerlink\" href=\"#loss-functions\" title=\"Link to this heading\">#</a></h3><p>For <span class=\"math notranslate nohighlight\">\\(K\\)</span>-class classification with one-hot labels, the <strong>cross-entropy</strong> loss is</p>", "a[href=\"#what-is-a-feed-forward-neural-network\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">1. What is a (feed-forward) neural network?<a class=\"headerlink\" href=\"#what-is-a-feed-forward-neural-network\" title=\"Link to this heading\">#</a></h2><p>We observe a dataset <span class=\"math notranslate nohighlight\">\\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n\\)</span> with inputs <span class=\"math notranslate nohighlight\">\\(x_i \\in \\mathbb{R}^d\\)</span> and labels <span class=\"math notranslate nohighlight\">\\(y_i\\)</span> (e.g., <span class=\"math notranslate nohighlight\">\\(y_i \\in \\{0,\\dots,9\\}\\)</span> for example CIFAR-10). A feed-forward neural network is a <strong>parametric function</strong></p>", "a[href=\"#optimization-by-stochastic-gradient-descent\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">3.3 Optimization by (stochastic) gradient descent<a class=\"headerlink\" href=\"#optimization-by-stochastic-gradient-descent\" title=\"Link to this heading\">#</a></h3><p>We update parameters by</p>", "a[href=\"#neural-networks-universal-approximation-and-feature-extraction-with-a-cifar-10-mlp-pytorch\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\">Neural Networks, Universal Approximation, and Feature Extraction (with a CIFAR-10 MLP) (PyTorch)<a class=\"headerlink\" href=\"#neural-networks-universal-approximation-and-feature-extraction-with-a-cifar-10-mlp-pytorch\" title=\"Link to this heading\">#</a></h1><p>This notebook introduces the core math behind feed-forward neural networks, the <strong>Universal Approximation Theorem (UAT)</strong>, essentials shared by (almost) all neural networks (problem setup, losses, optimization), and the idea of <strong>feature extraction</strong> as learned representations. We finish by training a simple <strong>fully connected network</strong> (no convolutions) on <strong>CIFAR-10</strong> to make the concepts concrete.</p>", "a[href=\"#regularization-generalization\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">3.4 Regularization &amp; generalization<a class=\"headerlink\" href=\"#regularization-generalization\" title=\"Link to this heading\">#</a></h3><p>Typical techniques:</p>", "a[href=\"#problem-statement-empirical-risk-minimization-erm\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">3.1 Problem statement &amp; Empirical Risk Minimization (ERM)<a class=\"headerlink\" href=\"#problem-statement-empirical-risk-minimization-erm\" title=\"Link to this heading\">#</a></h3><p>Given a parametric model <span class=\"math notranslate nohighlight\">\\(f_\\theta\\)</span> and loss <span class=\"math notranslate nohighlight\">\\(\\ell(\\cdot,\\cdot)\\)</span>, we minimize the <strong>empirical risk</strong>:</p>", "a[href=\"#universal-approximation-theorem-uat\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">2. Universal Approximation Theorem (UAT)<a class=\"headerlink\" href=\"#universal-approximation-theorem-uat\" title=\"Link to this heading\">#</a></h2><p><strong>Informal statement.</strong> A feed-forward network with a <strong>single hidden layer</strong> and a <strong>suitable nonlinearity</strong> (e.g., sigmoidal, or modern choices like ReLU under similar conditions) can approximate any <strong>continuous function</strong> on a <strong>compact set</strong> to <strong>arbitrary accuracy</strong>, provided it has <strong>enough hidden units</strong>.</p><p><strong>Meaning.</strong> Neural nets are <em>expressive enough</em> to fit (approximate) essentially any continuous target you might care about. However, UAT says <strong>nothing</strong> about:</p>"}
skip_classes = ["headerlink", "sd-stretched-link"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(`main ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: false,
                placement: 'auto-start', maxWidth: 500, interactive: true, boundary: document.body, appendTo: document.body,
                onShow(instance) {MathJax.typesetPromise([instance.popper]).then(() => {});},
            });
        };
    };
    console.log("tippy tips loaded!");
};
